<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Windows で RTX5070Ti を使って gpt-oss を動かす - hoge blog</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Windows で RTX5070Ti を使って gpt-oss を動かす" />
<meta property="og:description" content="windows で huggingface transformers を使って gpt-oss を動かすのにハマったのでメモ。 TL;DR RTX5070Ti の VRAM 16GB で gpt-oss を動かすには triton 3.4.0 と triton-kernel が必須 triton は linux のみの対応なので windows native は不可能。 wsl が必須 チャットするだけなら LM studio などのチャット専用ツールを利用するのが手間もなく簡単 gpt-oss OpenAI GPT3 以降の初のオープン な LLM である gpt-oss がリリースされたとのこと。 このネタは、 以下の内容から transformers を使ってローカル実行する部分を実施した際のメモになっている。 &lt;https://huggingface.co/blog/welcome-openai-gpt-oss&gt; 上記には、 transformers 以外にも llama.cpp, vLLM transformers serve が 紹介されて" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ifritjp.github.io/blog2/public/posts/2025/2025-08-07-gpt-oss-windows/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-08-07T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-08-07T00:00:00+00:00" />

		<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Windows で RTX5070Ti を使って gpt-oss を動かす"/>
<meta name="twitter:description" content="windows で huggingface transformers を使って gpt-oss を動かすのにハマったのでメモ。 TL;DR RTX5070Ti の VRAM 16GB で gpt-oss を動かすには triton 3.4.0 と triton-kernel が必須 triton は linux のみの対応なので windows native は不可能。 wsl が必須 チャットするだけなら LM studio などのチャット専用ツールを利用するのが手間もなく簡単 gpt-oss OpenAI GPT3 以降の初のオープン な LLM である gpt-oss がリリースされたとのこと。 このネタは、 以下の内容から transformers を使ってローカル実行する部分を実施した際のメモになっている。 &lt;https://huggingface.co/blog/welcome-openai-gpt-oss&gt; 上記には、 transformers 以外にも llama.cpp, vLLM transformers serve が 紹介されて"/>

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/blog2/public/css/style.css">
	

	<link rel="shortcut icon" href="/blog2/public/favicon.ico">
		
    <link rel="stylesheet" href="/blog2/public/css/highlight_lns.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
<script src="/blog2/public/js/highlight_lns.js"></script>
<script src="/blog2/public/js/hook.js"></script>
<link rel="stylesheet" href="/blog2/public/css/custom.css">


<script async src="https://www.googletagmanager.com/gtag/js?id=G-4708B8S6ES"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-4708B8S6ES');
</script>
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo logo--mixed">
		<a class="logo__link" href="/blog2/public/" title="hoge blog" rel="home">
			<div class="logo__item logo__imagebox">
					<img class="logo__img" src="/blog2/public/avatar.png">
				</div><div class="logo__item logo__text">
					<div class="logo__title">hoge blog</div>
					<div class="logo__tagline">生涯現役エンジニア</div>
				</div>
		</a>
	</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/blog2/public/">
				
				<span class="menu__text">Home</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/blog2/public/list.html">
				
				<span class="menu__text">Articles</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="https://ifritjp.github.io/documents/">
				
				<span class="menu__text">公開技術情報</span>
				
			</a>
		</li>
	</ul>
</nav>

	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Windows で RTX5070Ti を使って gpt-oss を動かす</h1>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class="meta__text" datetime="2025-08-07T00:00:00Z">2025-08-07</time></div></div>
		</header>
		
<div class="post__toc toc">
	<div class="toc__title">Page content</div>
	<div class="toc__menu">
		<nav id="TableOfContents">
<ul>
<li><a href="#headline-1">TL;DR </a>
</li>
<li><a href="#headline-2">gpt-oss </a>
</li>
<li><a href="#headline-3">16GB VRAM で動かす条件</a>
</li>
<li><a href="#headline-4">uv 用の pyproject.toml</a>
</li>
<li><a href="#headline-5">実行スクリプト</a>
</li>
<li><a href="#headline-6">Flash Attention 3 </a>
</li>
<li><a href="#headline-7">LM studio</a>
<ul>
<li><a href="#headline-8">proxy 環境下</a>
</li>
</ul>
</li>
</ul>
</nav>
	</div>
</div><div class="content post__content clearfix">
			
<p>
windows で huggingface transformers を使って gpt-oss を動かすのにハマったのでメモ。</p>
<div id="outline-container-headline-1" class="outline-2">
<h2 id="headline-1">
TL;DR 
</h2>
<div id="outline-text-headline-1" class="outline-text-2">
<ul>
<li>RTX5070Ti の VRAM 16GB で gpt-oss を動かすには triton 3.4.0 と triton-kernel が必須</li>
<li>triton は linux のみの対応なので windows native は不可能。 wsl が必須</li>
<li>チャットするだけなら LM studio などのチャット専用ツールを利用するのが手間もなく簡単</li>
</ul>
</div>
</div>
<div id="outline-container-headline-2" class="outline-2">
<h2 id="headline-2">
gpt-oss 
</h2>
<div id="outline-text-headline-2" class="outline-text-2">
<p>
OpenAI GPT3 以降の初のオープン な LLM である gpt-oss がリリースされたとのこと。</p>
<p>
このネタは、
以下の内容から transformers を使ってローカル実行する部分を実施した際のメモになっている。</p>
<p>
&lt;<a href="https://huggingface.co/blog/welcome-openai-gpt-oss">https://huggingface.co/blog/welcome-openai-gpt-oss</a>&gt;</p>
<p>
上記には、 transformers 以外にも llama.cpp, vLLM transformers serve が
紹介されているので、
気になる方はそちらを参考に。</p>
<p>
なお、 gpt-oss は 117B 版(gpt-oss-120b) と 21B 版(gpt-oss-20b)が公開されているが、
普通に考えてローカルで動かす場合 gpt-oss-20b になる。
H100 GPU を使えば gpt-oss-120b が動かせるということだが、
H100 GPU をローカルで動かせる人は圧倒的に少数だろう。</p>
<p>
なお、 gpt-oss-20b は 16GB の VRAM で動かせるが、
これには幾つか条件があるので注意が必要。</p>
</div>
</div>
<div id="outline-container-headline-3" class="outline-2">
<h2 id="headline-3">
16GB VRAM で動かす条件
</h2>
<div id="outline-text-headline-3" class="outline-text-2">
<p>
16GB VRAM で動かすには次の条件をクリアする必要がある。</p>
<ul>
<li>mxfp4 形式に対応した GPU</li>
<li>triton 3.4 と triton_kernels ライブラリ</li>
<li>pytorch 2.8</li>
</ul>
<p>まず 1 つめの mxfp4 形式に対応した GPU は、
どうやらコンシューマ向けでは RTX5000 シリーズだけらしい。</p>
<p>
次の triton 3.4 と triton_kernels は、 linux のみのサポートらしい。
(非公式の windows 版 triton もあるらしいが、未確認)</p>
<p>
つまり、 GPU が RTX5000 シリーズで、かつ linux に限定ということ。</p>
<p>
ただし linux の部分は、 linux ネイティブだけではなく wsl でも大丈夫。</p>
<p>
なお、何故 16GB VRAM を動かすのに上記条件が必要かというと、
mxfp4 形式で重みが保持されていて、これを扱う為の条件が上記になる。ということ。</p>
<p>
上記をクリアできない場合は bf16 として展開されるので、
mxfp4 に比べて少なくとも倍のサイズの VRAM が必要になる。</p>
<p>
なお、 mxfp4 でロードした時の VRAM の消費量が約 13.2GB なので、
単純に 2 倍にすると 26.4GB になる。</p>
<p>
コンシューマで VRAM 26.4GB をクリアしている GPU ってなにかあったか？</p>
</div>
</div>
<div id="outline-container-headline-4" class="outline-2">
<h2 id="headline-4">
uv 用の pyproject.toml
</h2>
<div id="outline-text-headline-4" class="outline-text-2">
<p>
そんな訳で、 
gpt-oss-20b を動かす環境を構築する際の uv 用 pyproject.toml を張っておく。</p>
<p>
なお、この pyproject.toml は jupyter notebook も含んでいるので、
不要なら削除してほしい。</p>
<p>
ただ、LLM のロードは時間がかかるので、 
notebook あるいは ipython を使った方が良いだろう。</p>
<div class="src src-toml">
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-toml" data-lang="toml"><span class="line"><span class="cl"><span class="p">[</span><span class="nx">project</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nx">name</span> <span class="p">=</span> <span class="s2">&#34;note-local&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nx">version</span> <span class="p">=</span> <span class="s2">&#34;0.1.0&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nx">description</span> <span class="p">=</span> <span class="s2">&#34;Add your description here&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nx">readme</span> <span class="p">=</span> <span class="s2">&#34;README.md&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nx">requires-python</span> <span class="p">=</span> <span class="s2">&#34;==3.12.*&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nx">dependencies</span> <span class="p">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;accelerate&gt;=1.9.0&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;ipywidgets&gt;=8.1.7&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;kernels&gt;=0.9.0&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;matplotlib&gt;=3.10.5&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;notebook&gt;=7.4.5&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;torch&gt;=2.8&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;transformers&gt;=4.55.0&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;triton&gt;=3.4.0&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;triton-kernels&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">[[</span><span class="nx">tool</span><span class="p">.</span><span class="nx">uv</span><span class="p">.</span><span class="nx">index</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl"><span class="nx">name</span> <span class="p">=</span> <span class="s2">&#34;pytorch-gpu&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nx">url</span> <span class="p">=</span> <span class="s2">&#34;https://download.pytorch.org/whl/cu128&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nx">explicit</span> <span class="p">=</span> <span class="kc">true</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="nx">tool</span><span class="p">.</span><span class="nx">uv</span><span class="p">.</span><span class="nx">sources</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nx">torch</span> <span class="p">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span> <span class="nx">index</span> <span class="p">=</span> <span class="s2">&#34;pytorch-gpu&#34;</span>  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nx">torchvision</span> <span class="p">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span> <span class="nx">index</span> <span class="p">=</span> <span class="s2">&#34;pytorch-gpu&#34;</span>  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nx">torchaudio</span> <span class="p">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span> <span class="nx">index</span> <span class="p">=</span> <span class="s2">&#34;pytorch-gpu&#34;</span>  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nx">triton</span> <span class="p">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span> <span class="nx">index</span> <span class="p">=</span> <span class="s2">&#34;pytorch-gpu&#34;</span>  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nx">triton-kernels</span> <span class="p">=</span> <span class="p">{</span> <span class="nx">git</span> <span class="p">=</span> <span class="s2">&#34;https://github.com/triton-lang/triton.git&#34;</span><span class="p">,</span> <span class="nx">subdirectory</span> <span class="p">=</span> <span class="s2">&#34;python/triton_kernels&#34;</span><span class="p">,</span> <span class="nx">rev</span> <span class="p">=</span> <span class="s2">&#34;main&#34;</span> <span class="p">}</span></span></span></code></pre></td></tr></table>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-5" class="outline-2">
<h2 id="headline-5">
実行スクリプト
</h2>
<div id="outline-text-headline-5" class="outline-text-2">
<p>
上記の uv の環境で notebook を起動し、
以下を実行すると &#34;How many rs are in the word &#39;strawberry&#39;?&#34; に対する回答が得られる。</p>
<div class="src src-py">
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model_id</span> <span class="o">=</span> <span class="s2">&#34;openai/gpt-oss-20b&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">model_id</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">device_map</span><span class="o">=</span><span class="s2">&#34;auto&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch_dtype</span><span class="o">=</span><span class="s2">&#34;auto&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="s2">&#34;role&#34;</span><span class="p">:</span> <span class="s2">&#34;user&#34;</span><span class="p">,</span> <span class="s2">&#34;content&#34;</span><span class="p">:</span> <span class="s2">&#34;How many rs are in the word &#39;strawberry&#39;?&#34;</span><span class="p">},</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">messages</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&#34;pt&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">generated</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">generated</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&#34;input_ids&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:]))</span></span></span></code></pre></td></tr></table>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-6" class="outline-2">
<h2 id="headline-6">
Flash Attention 3 
</h2>
<div id="outline-text-headline-6" class="outline-text-2">
<p>
上記スクリプトは通常版のモデルを実行するケースで、
それとは別に Flash Attention 3 という高速化版も使える。</p>
<p>
が、これは動かせなかった。</p>
<p>
サイトには以下の記述がある。</p>
<div class="src src-txt">
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-txt" data-lang="txt"><span class="line"><span class="cl">At the time of writing, 
</span></span><span class="line"><span class="cl">this super-fast kernel has been tested on Hopper cards with PyTorch 2.7 and 2.8.
</span></span><span class="line"><span class="cl">We expect increased coverage in the coming days. 
</span></span><span class="line"><span class="cl">----</span></span></code></pre></td></tr></table>
</div>
</div>
</div>
<p>
これを翻訳すると以下。</p>
<hr>
<p>
本稿執筆時点では、
この超高速カーネルはPyTorch 2.7および2.8を搭載した Hopper カードでテスト済みです。
今後数日中にカバレッジが拡大すると予想されます。</p>
<hr>
<p>
つまり、 Flash Attention 3 は Hopper の対応のみで
 RTX 5000 シリーズにはまだ対応していない、
ということなんだろうか？</p>
<p>
Flash Attention 3 は通常版に比べて高速になるということなので、期待したい。</p>
</div>
</div>
<div id="outline-container-headline-7" class="outline-2">
<h2 id="headline-7">
LM studio
</h2>
<div id="outline-text-headline-7" class="outline-text-2">
<p>
ついでに LM studio でも動かしてみた。</p>
<p>
transformers では実質的に RTX5000 が必須だったが、
LM studio では RTX 5000 シリーズでなくても普通に動かせる。
かつ、VRAM は 16GB も必要ない。
というか、CPU だけでも十分実用に使える速度で動かせる。
逆に、どうして GPU 使ってもこれだけしか速くならないんだと、残念に思う。</p>
<p>
実際に自分の環境で動かした結果は次の通り。</p>
<ul>
<li>
<p>GPU (RTX5070Ti) </p>
<ul>
<li>42.62 tok/sec</li>
</ul>
</li>
<li>
<p>CPU (Ryzen9 9900X / DDR5 5600)</p>
<ul>
<li>16.76 tok/sec</li>
</ul>
</li>
</ul>
<p>CPU だけでこれだけの速度で動き、
かつ、ハルシネーションのことを考えなければ非常に優秀なモデルなので、
とりあえず PC にインストールしておいて損は無い。</p>
<p>
特に英語学習(reading,writing)に関して言えば、もやはこれだけあれば十分な気がする。</p>
<p>
また、 OpenAI のモデルなのでトレーニングデータセット的にも安心できるため、
モデルデータをダウンロードしたところで面倒なことは起らないだろう。</p>
<p>
なお、展開後の RAM 使用量は約 13GB なので、
CPU を利用する場合のシステムメモリは最低でも 32GB。
とはいえ 32GB だと本当にギリギリなラインなので、 64GB は欲しいところ。</p>
<p>
以上を踏まえると、次のように運用するのが良さそうだ。</p>
<p>
集中して LLM を動かす場合のみ GPU を割り当て、
他に AI のトレーニングやゲームなどで GPU を使う場合は、
LLM を CPU 単体で動かす。</p>
<div id="outline-container-headline-8" class="outline-3">
<h3 id="headline-8">
proxy 環境下
</h3>
<div id="outline-text-headline-8" class="outline-text-3">
<p>
proxy 環境下だと LM studio からモデルのダウンロードが出来ないので、
次の手順に従ってモデルのインポートを行なうことで使えるようになる。</p>
<p>
&lt;<a href="https://lmstudio.ai/docs/app/basics/import-model">https://lmstudio.ai/docs/app/basics/import-model</a>&gt;</p>
<p>
このときに利用する lms コマンドは以下にある。</p>
<pre class="example">
&#34;C:\Users\?????\.lmstudio\bin\lms.exe&#34;
</pre>
<p>
また、 gpt-oss-20b モデルは以下の URL のものを事前にダウンロードしておく。</p>
<p>
&lt;<a href="https://huggingface.co/lmstudio-community/gpt-oss-20b-GGUF">https://huggingface.co/lmstudio-community/gpt-oss-20b-GGUF</a>&gt;</p>
</div>
</div>
</div>
</div>

		</div>
	</article>
</main>

<div class="authorbox clearfix">
	<figure class="authorbox__avatar">
		<img alt="ifritJP avatar" src="/blog2/public/avatar.png" class="avatar" height="90" width="90">
	</figure>
	<div class="authorbox__header">
		<span class="authorbox__name">About ifritJP</span>
	</div>
	<div class="authorbox__description">
		生涯現役エンジニア
	</div>
</div>

<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/blog2/public/posts/2025/2025-07-27-how-to-develop-comfyui-node/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">ComfyUI node の開発方法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/blog2/public/posts/2025/2025-08-30-tf-rtx5000/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">TensorFlow を Geforce RTX5000 シリーズで動かす</p>
		</a>
	</div>
</nav>


			</div>
			<aside class="sidebar">
<div class="widget-social widget">
	<h4 class="widget-social__title widget__title">Social</h4>
	<div class="widget-social__content widget__content">
		<div class="widget-social__item widget__item">
			<a class="widget-social__link widget__link btn" title="Twitter" rel="noopener noreferrer" href="https://twitter.com/dwarfjp" target="_blank">
				<svg class="widget-social__link-icon icon icon-twitter" width="24" height="24" viewBox="0 0 384 312"><path d="m384 36.9c-14.1 6.3-29.3 10.5-45.2 12.4 16.3-9.7 28.8-25.2 34.6-43.6-15.2 9-32.1 15.6-50 19.1-14.4-15.2-34.9-24.8-57.5-24.8-43.5 0-78.8 35.3-78.8 78.8 0 6.2.7 12.2 2 17.9-65.5-3.3-123.5-34.6-162.4-82.3-6.7 11.6-10.6 25.2-10.6 39.6 0 27.3 13.9 51.4 35 65.6-12.9-.4-25.1-4-35.7-9.9v1c0 38.2 27.2 70 63.2 77.2-6.6 1.8-13.6 2.8-20.8 2.8-5.1 0-10-.5-14.8-1.4 10 31.3 39.1 54.1 73.6 54.7-27 21.1-60.9 33.7-97.8 33.7-6.4 0-12.6-.4-18.8-1.1 34.9 22.4 76.3 35.4 120.8 35.4 144.9 0 224.1-120 224.1-224.1 0-3.4-.1-6.8-.2-10.2 15.4-11.1 28.7-25 39.3-40.8z"/></svg>
				<span>Twitter</span>
			</a>
		</div>
		<div class="widget-social__item widget__item">
			<a class="widget-social__link widget__link btn" title="GitHub" rel="noopener noreferrer" href="https://github.com/ifritJP" target="_blank">
				<svg class="widget-social__link-icon icon icon-github" width="24" height="24" viewBox="0 0 384 374"><path d="m192 0c-106.1 0-192 85.8-192 191.7 0 84.7 55 156.6 131.3 181.9 9.6 1.8 13.1-4.2 13.1-9.2 0-4.6-.2-16.6-.3-32.6-53.4 11.6-64.7-25.7-64.7-25.7-8.7-22.1-21.3-28-21.3-28-17.4-11.9 1.3-11.6 1.3-11.6 19.3 1.4 29.4 19.8 29.4 19.8 17.1 29.3 44.9 20.8 55.9 15.9 1.7-12.4 6.7-20.8 12.2-25.6-42.6-4.8-87.5-21.3-87.5-94.8 0-20.9 7.5-38 19.8-51.4-2-4.9-8.6-24.3 1.9-50.7 0 0 16.1-5.2 52.8 19.7 15.3-4.2 31.7-6.4 48.1-6.5 16.3.1 32.7 2.2 48.1 6.5 36.7-24.8 52.8-19.7 52.8-19.7 10.5 26.4 3.9 45.9 1.9 50.7 12.3 13.4 19.7 30.5 19.7 51.4 0 73.7-44.9 89.9-87.7 94.6 6.9 5.9 13 17.6 13 35.5 0 25.6-.2 46.3-.2 52.6 0 5.1 3.5 11.1 13.2 9.2 76.2-25.5 131.2-97.3 131.2-182 0-105.9-86-191.7-192-191.7z"/></svg>
				<span>GitHub</span>
			</a>
		</div>

		
	</div>
</div>
<div class="widget-recent widget">
	<h4 class="widget__title">Recent Posts</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/blog2/public/posts/2025/2025-08-30-tf-rtx5000/">TensorFlow を Geforce RTX5000 シリーズで動かす</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog2/public/posts/2025/2025-08-07-gpt-oss-windows/">Windows で RTX5070Ti を使って gpt-oss を動かす</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog2/public/posts/2025/2025-07-27-how-to-develop-comfyui-node/">ComfyUI node の開発方法</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog2/public/posts/2025/2025-07-20-comfyui-guide/">ComfyUI のインストールと使いこなしに必要な Diffusion モデル概要入門</a></li>
			<li class="widget__item"><a class="widget__link" href="/blog2/public/posts/2025/2025-07-06-comfyui-performance-on-win/">Windows で RTX5070 TI を使った ComfyUI のパフォーマンス</a></li>
		</ul>
	</div>
</div>
</aside>
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2025 hoge blog.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/blog2/public/js/menu.js"></script>
</body>
</html>
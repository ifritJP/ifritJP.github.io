{"categories":[],"posts":[{"content":" イマドキは少数派だと思うが、 PC に ubuntu と windows のデュアルブートを設定している。  さらに面倒なことに、 windows は BitLocker で暗号化 \u0026amp; PIN 認証を設定している。  そして、この状態で ubuntu を apt upgrade したら、 何故か windows ブート時の BitLocker の PIN 認証が失敗するようになった。  PIN を間違えているはずはないのだが、何度やっても PIN 認証が通らない。  しかたがないので、 BitLocker の回復キーを入力したところ問題なく起動した。  そういえば ubuntu の apt upgrade 実行時、grub の更新が掛った。 その際、設定をどうするか聞いてきたので、 設定を変更しないように選択したのだが何か問題があったようだ。  なお、回復キーを使って起動した後、 再度 PIN を設定することで、問題なく PIN 認証が通るようになった。  PIN を忘れてなくても、回復キーが必要になることがあるんだな。 ","id":0,"section":"posts","summary":"イマドキは少数派だと思うが、 PC に ubuntu と windows のデュアルブートを設定している。 さらに面倒なことに、 windows は BitLocker で暗号化 \u0026amp; PIN 認証を設定している。 そして、この","tags":null,"title":"デュアルブートの ubuntu を upgrade したら windows の BitLocker が PIN の認証失敗するようになった","uri":"https://ifritjp.github.io/blog2/public/posts/2020/2020-07-10-bitlocker/","year":"2020"},{"content":"  Go の勉強を兼て「これ」を Go で作っていたんだが、その時感じた Go の特徴をまとめておく。  Go は気軽に書けるのに、非常に高い実行パフォーマンスを出せる使い勝手の良い言語だと思う。  また、パッケージマネージャを言語自身に内蔵しているため、 拡張パッケージが揃っていて、今後さらにパッケージが充実して使える言語になるだろう。  こんな様なことは、もう誰もが書いていることだと思うので、 以降では、もう少し違った角度で Go について考えたことをまとめておく。 Go はエンジニアを信用している言語   「エンジニアを信用している」 とはどういう事かというと、 Go を使うエンジニアはつまらない間違いをしない高レベルな技術を持っていることを 前提にしている、ということだ。  この根拠は、Go の次の言語仕様から来ている。    nil 安全がない    構造体のコンストラクタがない    アクセス制限が公開と package 内限定しかない    Generics がない    基本的に mutable    shadowing 可能    排他制御が古典的    静的型付け言語は、安全方向に仕様を振っていて、 出来ることを制限する手段を提供していることが多い(例えばアクセス制限や Generics 等)。 一方 Go では、そのような制限する手段を提供していない部分が多い。  では何故、Go は言語仕様による制限をしないのか？ それは、 Go の設計者が、そんな機能に頼らなくても安全に開発を進められる、 という思いがあったからだろう。  一方近年話題になっている Rust では、 Go とは逆にエンジニアを信用していない言語で、 エンジニアはヒューマンエラーを起すことを前提にしている。 そして、ヒューマンエラーが起きたときは、コンパイラレベルで検知して エラーするようにしている。ただ、これを実現するために多くのメタ情報を コード上に宣言する必要がある。  これは言語仕様の決め方の方向性が違うだけで、 どちらが正解で、どちらが間違っているというものではない。  プロジェクトで採用する言語を決定する際に、 どのような言語がそのプロジェクトにマッチするのかを判断することが重要だ。 ","id":1,"section":"posts","summary":"Go の勉強を兼て「これ」を Go で作っていたんだが、その時感じた Go の特徴をまとめておく。 Go は気軽に書けるのに、非常に高い実行パフォーマンスを出せる","tags":null,"title":"Go 言語 (golang) について思ったこと","uri":"https://ifritjp.github.io/blog2/public/posts/2020/2020-06-27-golang/","year":"2020"},{"content":" 以前 lunescript の紹介記事を書いている時に、 lunescript の日本語訳がふと気になったんで調べていたんだが、 その時の Google 翻訳の結果が衝撃的だった。  \u0026lt;https://ifritjp.github.io/documents/lunescript/tutorial1/#headline-3\u0026gt;  で、久し振りに Google 翻訳で lunescript を翻訳してみた。 その結果は次の通り。   めでたく lunescript の日本語訳が lunescript になった。  これは、 LuneScript が Google に固有名詞として認識されたということだろうか？  それとも、該当する単語が登録されていないから、 とりあえずそのまま表示しているだけなんだろうか？ ","id":2,"section":"posts","summary":"以前 lunescript の紹介記事を書いている時に、 lunescript の日本語訳がふと気になったんで調べていたんだが、 その時の Google 翻訳の結果が衝撃的だった。 \u0026lt;https://ifritjp.github.io/documents/lunescript/tutorial1/#headline-3\u0026gt; で、久し振りに Google 翻","tags":null,"title":"LuneScript の Google 翻訳","uri":"https://ifritjp.github.io/blog2/public/posts/2020/2020-06-26-lunescript-trans/","year":"2020"},{"content":"  tunnel ツールのネタを書いた時、 dot を使ってグラフを作った。  dot は手軽にグラフを書ける便利なツールだが、 レイアウト制御に難があると思う。 グラフ作成ツールの利点と欠点   dot などのグラフ作成ツールの利点には次が挙げられる。  ノードのリンクを指定するだけで、後はツールが良い感じにグラフを自動で作成してくれる。  パワポ等でグラフを作成するのと比べると、これは大きな利点だ。  そして多くの場合、ツールが作成するグラフは、それなりに見易いグラフになってくれる。  ただ、少ない情報から自動でグラフを作成するため、 意図とは異なるレイアウトのグラフが出来あがることもある。  レイアウトのことは割り切って使うとか、 気に入らないなら他のパワポなどの draw 系のツールで描けば良いという話もあるが、 それは何か違うと思っている。 dot のグラフ   次の図は、 tunnel と host を繋ぐ処理をグラフ化したものだ。   このグラフの dot コードは次になる。 digraph G { rankdir = RL; tunnel [shape=doublecircle]; subgraph clusterA { packetWriter [shape=rect; margin=0.2;]; packetReader [shape=rect; margin=0.2;]; keepalive [shape=rect; margin=0.2;]; WriteQueue tunnel2Stream [shape=rect; margin=0.2;]; stream2Tunnel [shape=rect; margin=0.2;]; ReadQueue {rank = max; packetReader; packetWriter} {rank = same; WriteQueue; ReadQueue} {rank = min; tunnel2Stream; stream2Tunnel; keepalive} } host [shape=box3d]; tunnel -\u0026gt; packetReader packetReader -\u0026gt; ReadQueue ReadQueue -\u0026gt; tunnel2Stream stream2Tunnel -\u0026gt; WriteQueue WriteQueue -\u0026gt; packetWriter packetWriter -\u0026gt; tunnel keepalive -\u0026gt; WriteQueue tunnel2Stream -\u0026gt; host host -\u0026gt; stream2Tunnel {rank=min;host} }   コードの細かい部分はここでは触れないが、 次の 4 つの {rank=} を指定していることを確認して欲しい。    {rank = max; packetReader; packetWriter}    {rank = same; WriteQueue; ReadQueue}    {rank = min; tunnel2Stream; stream2Tunnel; keepalive}    {rank=min;host}    この rank 指定を外してグラフを生成すると次のようになる。 digraph G { rankdir = RL; tunnel [shape=doublecircle]; subgraph clusterA { packetWriter [shape=rect; margin=0.2;]; packetReader [shape=rect; margin=0.2;]; keepalive [shape=rect; margin=0.2;]; WriteQueue tunnel2Stream [shape=rect; margin=0.2;]; stream2Tunnel [shape=rect; margin=0.2;]; ReadQueue } host [shape=box3d]; tunnel -\u0026gt; packetReader packetReader -\u0026gt; ReadQueue ReadQueue -\u0026gt; tunnel2Stream stream2Tunnel -\u0026gt; WriteQueue WriteQueue -\u0026gt; packetWriter packetWriter -\u0026gt; tunnel keepalive -\u0026gt; WriteQueue tunnel2Stream -\u0026gt; host host -\u0026gt; stream2Tunnel }   rank 指定の有無の違い   rank 指定の有無によって生成されるグラフがどのように違いがあるのか、 分かり易いように並べて表示する。    rank 指定あり       rank 指定なし     rank 指定ありは矢印の向きが素直に円を描いる一方で、 rank 指定なしは矢印が交差していたり、矢印が長かったりで、 rank 指定ありと比べて動きが捉え辛くないだろうか？  このように、意図したレイアウトと異なる結果になった場合、 rank を指定することで、ある程度の制御が出来る。 rank 指定の意味   今回指定した 4 つの rank の内、次の 3 つは中央の四角の中の並び順を指定している。    {rank = max; packetReader; packetWriter}    {rank = same; WriteQueue; ReadQueue}    {rank = min; tunnel2Stream; stream2Tunnel; keepalive}    そもそも、 rank は何を指定するものなのかというと、 dot がリンク情報を元に どのノードをどこに配置するかを決定するアルゴリズムにおいて使用する要素の一つだ。  上記の 3 つの指定は、 packetReader, packetWriter が max のランクで、 WriteQueue, ReadQueue が同じランクで、 tunnel2Stream, stream2Tunnel, keepalive が min のランクであることを設定している。  これは、 rank 指定した時の図と見比べて、 中央の四角の中の左側から max, same, min の順で並べられていることから納得できる。  4 つの内の最後の rank 指定は、 host の場所を指定している。    {rank=min;host}    これは、 host が min のランクであることを設定している。  これも rank 指定した時の図と見比べて、 host が一番右に配置されていることから納得できる。  このように、 rank に max, same, min を指定することで、 ノードの配置を指定することが可能だ。  なお、 rank の指定は全部で 5 種類ある。    min    max    same    source    sink    これらの意味について、公式サイトに次の記載がある。 Rank constraints on the nodes in a subgraph. If rank=\u0026#34;same\u0026#34;, all nodes are placed on the same rank. If rank=\u0026#34;min\u0026#34;, all nodes are placed on the minimum rank. If rank=\u0026#34;source\u0026#34;, all nodes are placed on the minimum rank, and the only nodes on the minimum rank belong to some subgraph whose rank attribute is \u0026#34;source\u0026#34; or \u0026#34;min\u0026#34;. Analogous criteria hold for rank=\u0026#34;max\u0026#34; and rank=\u0026#34;sink\u0026#34;. (Note: the minimum rank is topmost or leftmost, and the maximum rank is bottommost or rightmost.)   min と source、 max と sink は同じように利用できる。  ただ、上記の記載にはないが、 min, max と souce, sink を混在して使用する際は、 注意が必要である。  なぜならば、min, max と souce, sink はそれぞれ異なる軸(X と Y)で 処理されるようなので、同じ軸でランク付けを行なう場合、 min, max, souce, sink を混在させてはならない。 ","id":3,"section":"posts","summary":"tunnel ツールのネタを書いた時、 dot を使ってグラフを作った。 dot は手軽にグラフを書ける便利なツールだが、 レイアウト制御に難があると思う。 グラフ作成ツー","tags":null,"title":"dot のレイアウト指定","uri":"https://ifritjp.github.io/blog2/public/posts/2020/2020-06-09-graph/","year":"2020"},{"content":"  go で proxy server を建てるには、 github.com/elazarl/goproxy を使うと簡単に実現できる。  https://github.com/elazarl/goproxy  github の readme を見れば、簡単な使い方が載っているので特に問題はないだろう。  ただ、一点だけハマったポイントがあるので書いておく。 proxy 環境下で goproxy を使う場合の注意点  package main import ( \u0026#34;github.com/elazarl/goproxy\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { proxy := goproxy.NewProxyHttpServer() proxy.Verbose = true log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, proxy)) }   github の readme にサンプルとして上記コードが載っている。  基本的にこれで問題ないのだが、 proxy 環境下で動かす場合には注意が必要だ。  多くの場合、 proxy 環境下では環境変数に次のような設定をしているだろう。 export HTTP_PROXY=http://proxy.hoge.com:80/ export HTTPS_PROXY=http://proxy.hoge.com:80/   このような設定を行なっている場合、 上記サンプルコードを動かすと、 goproxy はさらに proxy.hoge.com を使って接続を行なおうとする。  つまり、 goproxy を使って localhost:80 にアクセスしようとすると、次のような形になる。 client --\u0026gt; goproxy --\u0026gt; proxy.hoge.com:80 --\u0026gt; localhost:80   ここで問題なのは、 proxy.hoge.com:80 が間に挟まることで通信が確立できなくなる可能性がある、 ということだ。  少なくとも、goproxy にとっての localhost と、 proxy.hoge.com にとっての localhost は意味が異なるし、 プライベートアドレス IP 指定を受けつけない proxy も多いだろう。 対応策   前述した proxy 環境下の問題を回避するには、次の 2 つがある。    goproxy を使用する際に上記環境変数の設定を消す    goproxy を使うコードを修正する    goproxy を使うコードを修正するには、 次のように proxy.ConnectDial = nil を追加すれば良い。 package main import ( \u0026#34;github.com/elazarl/goproxy\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { proxy := goproxy.NewProxyHttpServer() proxy.Verbose = true proxy.ConnectDial = nil // これを追加  log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, proxy)) }  ","id":4,"section":"posts","summary":"go で proxy server を建てるには、 github.com/elazarl/goproxy を使うと簡単に実現できる。 https://github.com/elazarl/goproxy github の readme を見れば、簡単な使い方が載っているので特に問題はないだろう。 ただ、一点だけハマった","tags":null,"title":"go の proxy server (github.com/elazarl/goproxy) の使い方","uri":"https://ifritjp.github.io/blog2/public/posts/2020/2020-06-04-go-proxy/","year":"2020"},{"content":"  とある理由から 「Tunnel/Reverse Tunnel over websocket」 が必要になったので作ってみた。  「Tunnel/Reverse Tunnel over websocket」 が何かというと、 「websocket を tunnel にして別の TCP 通信を通すもの」だ。 「Tunnel/Reverse Tunnel over websocket」 とは   「Tunnel/Reverse Tunnel over websocket」を少し具体的にいうと、 次のような構成で通信を可能にするモノだ。 frame tunnelの例 { rectangle network_1 { node tcp_client_A node tunnel_client_1 } rectangle network_2 { node tunnel_server_1 node tcp_server_B } } tcp_client_A --\u0026gt; tunnel_client_1 tunnel_client_1 --\u0026gt; tunnel_server_1 tunnel_server_1 --\u0026gt; tcp_server_B tcp_client_A ..\u0026gt; tcp_server_B frame reverse_tunnelの例 { rectangle network_A { node tcp_server_C node tunnel_client_2 } rectangle network_B { node tunnel_server_2 node tcp_client_D } } tcp_server_C \u0026lt;-- tunnel_client_2 tunnel_client_2 --\u0026gt; tunnel_server_2 tunnel_server_2 \u0026lt;-- tcp_client_D tcp_client_D ..\u0026gt; tcp_server_C    上の図は network_1, network_2, network_A, network_B の 4 つのネットワークを表わしている。  このネットワーク間でポートが制限されていると、 tcp_client_A は tcp_server_B と直接通信が出来ない(図の点線)。  開放されているポートで接続し、そのセッション上に仮想的な Tunnel を構築する。 そして、その Tunnel 内にポートの制限を受けない通信を実現する。  左は Tunnel の構成例である。 ポート制限されている network_1, network_2 を、 tunnel server / tunnel client で接続して tunnel を構成し、 その tunnel を使って tcp client A と tcp server B を接続する。  右は Reverse Tunnel の構成例である。 ポート制限されている network_A, network_B を、 tunnel server / tunnel client で接続して tunnel を構成し、 その tunnel を使って tcp client D と tcp server C を接続する。  Tunnel と Revers Tunnel の違いは、 tcp client/server の位置関係である。  具体的には、 Tunnel server と同じネットワークに tcp server が属する構成が Tunnel で、 逆に Tunnel server と同じネットワークに tcp client が属する構成が Reverse Tunnel である。  そして、 Tunnel server と client 間の通信経路として、 websocket を利用して tunnel を構築するのが 「Tunnel/Reverse Tunnel over websocket」である。 VPN (Virtual Private Network)   このように制限されたネットワーク間で通信路を構築する方法として、 WireGuard や OpenVPN などの低レイヤー VPN がある。  低レイヤー VPN は、その名の通り仮想的なネットワークを低レイヤーで構築する。 これによって、通常のネットワークと同様に扱えて利便性が高いが、 通常のネットワークと同様であるが故、逆にリスクになる可能性がある。  今回は VPN ではなく、Tunnel を実現するのが目的である。  なお、ここでは「レイヤー 2 あるいは 3 を仮想化する技術」を VPN とし、 「ネットワーク間で TCP セッションを転送する技術」を Tunnel とする。 開発した背景   制限されたネットワーク間での通信を確立できないかどうか、 当初はフリーのツールを探して tunnel ソフトを幾つか試してみたが、 tunnel が接続できなかったり、接続できてもすぐに切れてしまったりで イマイチ希望したものとは違った。  特に自分の環境は (A)/(B) 間のネットワーク環境が悪く、 tunnel を確立しても、ある程度経過すると切断されてしまう問題があった。  tunnel が切断されても tunnel を再接続することで、 tunnel 内の tcp 通信を継続させることは論理的に可能だ。 しかし、検討していた幾つかのフリーのツールでは、 tunnel が切断されると tunnel 自体のを再接続が出来ても tunnel 内を流れる tcp 通信が継続できなかった。 そもそも tunnel を再接続すること自体、 成功したり失敗したりしているような状況だった。  そこで、今回はフリーのツールを検討することは諦め、 自分の勉強も兼てスクラッチで開発することにした。  なお、ネットワーク間を接続することが目的であれば、 WireGuard や OpenVPN などの低レイヤー VPN や、 stunnel などの Tunnel ツールを利用するのが多くの場合ベストだろう。 ネットワーク環境   今回開発した Tunnel ツールを使って、 自分のネットワーク環境の Tunnel 間通信強制切断状況を確認したところ、 次のようになった。   (a)    接続は最大でも 15 分程度で切断される   (b)    昼間は 30 秒程度で切断される   (c)    接続の 7 割強は 1 分以内で切断される    (a) について、 どうやら自分の環境では http 通信は 15 分程度でセッションが強制切断されるらしい。  (b), (c) について、 無通信が続くと 30 〜 60 秒程度で強制切断されるようなので、 無通信を回避するために 20 秒毎にトンネル間でダミーの通信を行なうよう対応した。  ただ、これでも通信負荷が高くなると数分で切断されることがある。 使用方法   このツールは Go で開発しているため、 事前に Go(1.14.2) の環境を構築してあることが前提である。 注意事項   tunnel 間の通信がインターネットを経由する場合、セキュリティには十分注意すること。    tunnel client/server 間通信の暗号化や、client 認証を実装しているが、 tunnel 内の TCP セッションは raw な tcp 接続をせずに、 ssh などで接続すること。    tunnel server は常駐させず、必要な時にだけ起動するように運用すること。    pass , encPass オプションを必ず指定し、適切な期間で変更すること。    ip オプションを指定し、接続可能な client を制限すること。   ビルド   次のコマンドを実行することで、 tunnel ディレクトリ内に tunnel コマンドがカレントディレクトリに生成される。 $ git clone --depth 1 https://github.com/ifritJP/kptunnel.git $ cd kptunnel $ make build  kptunnel コマンド   kptunnel コマンドは tunnel server と、 tunnel client の両方の役割を持ち、 オプションで切り替える。  kptunnel コマンドは、次の書式をもつ。 $ kptunnel \u0026lt;mode\u0026gt; \u0026lt;server\u0026gt; [forward [forward [...]]] [options]     mode    次のいずれかを指定する    サーバ    wsserver    r-wsserver    server    r-server      クライアント    wsclient    r-wsclient    client    r-client      \u0026#34;r-\u0026#34; が付くものは、 reverse tunnel である。    ws が付くものは、 over websocket である。    ws が付かないものは、 tcp で直接接続する。    tcp による接続は、実験的なサポートである。    tcp で接続できる環境なら、 このツールを使わずに ssh した方が良いだろう。      \u0026#34;r-\u0026#34;, \u0026#34;ws\u0026#34; は client/server で一致している必要がある。      server    server を示す。    サーバ側で指定する場合は、開放するポートを指定する。 (:1234 or localhost:1234)    この port に接続可能なネットワークを制限する場合は、 そのネットワークを指定する。 例えば localhost に制限する場合は localhost:1234 として指定する。      クライアント側で指定する場合は、ホスト名を含めて指定する (hoge.com:1234)      forward    tunnel で forward するポートの情報。    forward は複数指定できる。    reverse tunnel の場合は、 server 側で指定する。tunnel の場合は client 側で指定する。    \u0026#34;localのポート,forward先のポート\u0026#34; の書式で指定する。    localのポートに接続可能なネットワークを制限する場合は、 そのネットワークを指定する。 例えば localhost に制限する場合は localhost:1234 として指定する。    forward 先のポート情報は、相手にそのまま伝わる。    例えば reverse tunnel で localhost を指定した場合、localhost は tunnel クライアント自身になり、 通常の tunnel の場合、 localhost はサーバ自身になる。        次に代表的なコマンド例を示す。    server    server のコマンド例を示す。 $ kptunnel r-wsserver :6666 :8001,localhost:22 -pass XXXXXXX -encPass YYYYYYYY   これは次のサーバの実行を指定している。    option 意味 サンプルの意味     r-wsserver client/server の種類 reverse websocket server   :6666 tunnel サーバの情報 ポート 6666 を使用して websocket server を建てる   :8001,localost:22 tunnel で forward するポート番号 server の 8001 を client の localhost:22 に forward   -pass client の認証用パスワード XXXXXXX   -encPass client/server 間の通信路の暗号パスワード YYYYYYYY       client    client のコマンド例を示す $ kptunnel r-wsclient hoge.hoge.com:80 -proxy http://user:pass@proxy.hoge.com:8080/ -pass XXXXXXX -encPass YYYYYYYY   これは次のクライアントの実行を指定している。    option 意味 サンプルの意味     r-wsclient client/server の種類 reverse websocket client   hoge.hoge.com:80 tunnel サーバの情報 hoge.hoge.com の 80 に接続する   -proxy proxy サーバの情報 http://proxy.hoge.com::8080/ に user, pass で接続   -pass client の認証用パスワード XXXXXXX   -encPass client/server 間の通信路の暗号パスワード YYYYYYYY       tunnel への接続    上記のサンプルは localhost の 22 番ポートに接続するための reverse tunnel を構築している。 つまり、このサーバ側の 8001 ポートに繋げると、 client 側の ssh に接続されることになる。  よって、サーバ側で次のコマンドを実行することで、クライアントの ssh に接続できる。 $ ssh -p 8001 localhost  オプション一覧   kptunnel コマンドで使用可能なオプションについて説明する 基本     -proxy string    websocket server に接続するための proxy    proxy 不要なら省略する。    認証が必要な proxy の場合、 http://user:pass@proxy.hoge.com:port/ の形式で指定する。    現状は HTTP proxy のみ対応している。    client 側で指定する      -UA string    Proxy に接続する際の User Agent を指定する    websocket の client で有効     セキュリティ関連     -pass string    client 認証で使用する。    client/server で共通のものを指定する必要がある。    client 認証は challenge/respose で行なう。      -encPass string    client/server 間通信の暗号パスワード。    client/server で共通のものを指定する必要がある。      -encCount int    client/server 間の暗号処理回数を指定する。 (default -1)    -1 : infinity    0 : plain, no encrypt.    N \u0026gt; 0 : packet count      このツールは tunnel client/server 間の通信を暗号化するが、tunnel 内を通すのが ssh などの場合、 二度の暗号化が走ることになり、tunnel client/server 間の暗号は無駄になる。 そこで、tunnel client/server 間の暗号化回数を指定することで、暗号化にかかる負荷軽減を可能にする。    回数は tunnel の通信パケット単位    暗号アルゴリズムは AES256 CFB を使用している。      -ip string    server に接続可能な client の、 IP アドレス範囲を指定する。    e.g. 192.168.0.0/24      このオプションを省略した場合、 client の IP を限定しない。     動作デモ   次を実行しているデモ動画を示す。    remote と local と、それらを仲介する proxy がある。    remote で tunnel の wsserver を起動    proxy を起動    local から wsclient を使って、proxy 経由で remote と tunnel を構築する    local から tunnel 経由で remote と ssh 接続する    ssh のコンソースから X11 アプリ (ico) を起動    proxy を停止    tunnel が切断される    X11 アプリ (ico) の更新が止まるが、 ssh のセッションは継続する      proxy を起動    tunnel が再接続される    ssh のセッションが再開する    X11 アプリ (ico) の更新が再開する      以降 proxy 停止、起動を繰り返し    開発に関して   これ以降の章では、この Tunnel ツール開発に関する技術的な内容について記載する。 スレッド   この Tunnel ツールは、主に次の 6 つのスレッドで構成される。    tunnel session 制御    WriteQeue → tunnel のパケット送信制御 (packetWriter)    tunnel → ReadQueue のパケット受信制御 (packetReader)    ReadQueue → host のパケット転送制御 (tunnel2Stream)    WriteQeue → tunnel のパケット転送制御 (stream2Tunnel)    無通信が一定時間続かないようにするダミーパケット送信制御 (keepalive)    スレッド多す過ぎという気もするが、 メニーコア時代な現代であれば、 少ないスレッドで複雑なコードを書くよりも、 処理毎にスレッドを分けた方がメンテナンス性も性能も良いんじゃないだろうか？  下図は、各スレッドの役割を図示している。 digraph G { rankdir = RL; tunnel [shape=doublecircle]; subgraph clusterA { packetWriter [shape=rect; margin=0.2;]; packetReader [shape=rect; margin=0.2;]; keepalive [shape=rect; margin=0.2;]; WriteQueue tunnel2Stream [shape=rect; margin=0.2;]; stream2Tunnel [shape=rect; margin=0.2;]; ReadQueue {rank = max; packetReader; packetWriter} {rank = same; WriteQueue; ReadQueue} {rank = min; tunnel2Stream; stream2Tunnel; keepalive} } host [shape=box3d]; tunnel -\u0026gt; packetReader packetReader -\u0026gt; ReadQueue ReadQueue -\u0026gt; tunnel2Stream stream2Tunnel -\u0026gt; WriteQueue WriteQueue -\u0026gt; packetWriter packetWriter -\u0026gt; tunnel keepalive -\u0026gt; WriteQueue tunnel2Stream -\u0026gt; host host -\u0026gt; stream2Tunnel {rank=min;host} }      packetReader は tunnel からデータを読み取り ReadQueue に送る    tunnel2Stream は ReadQueue からデータを読み取り host に送る    stream2Tunnel は host からデータを読み取り WriteQueue に送る    packetWriter は WriteQueue からデータを読み取り tunnel に送る    keepalive は WriteQueue にダミーデータを送る   tunnel 内に複数の TCP セッションを通す場合   tunnel には複数の TCP セッションを通すことができる。 次の要素は、tunnel 内の TCP セッション毎に増える。    tunnel2Stream    stream2Tunnel    ReadQueue    これらをまとめて CITI (connection in tunnel information ) とすると、 2 つの TCP セッションを通す場合は次のような構成になる。 digraph G { rankdir = RL; tunnel [shape=doublecircle]; subgraph clusterA { packetWriter [shape=rect; margin=0.2;]; packetReader [shape=rect; margin=0.2;]; keepalive [shape=rect; margin=0.2;]; WriteQueue CITI1 [shape=component; margin=0.2;]; CITI2 [shape=component; margin=0.2;]; {rank = max; packetReader; packetWriter} {rank = same; WriteQueue; } {rank = min; CITI1; CITI2; keepalive} } host1 [shape=box3d]; host2 [shape=box3d]; tunnel -\u0026gt; packetReader WriteQueue -\u0026gt; packetWriter packetWriter -\u0026gt; tunnel keepalive -\u0026gt; WriteQueue packetReader -\u0026gt; CITI1 CITI1 -\u0026gt; host1 CITI1 -\u0026gt; WriteQueue host1 -\u0026gt; CITI1 packetReader -\u0026gt; CITI2 CITI2 -\u0026gt; host2 CITI2 -\u0026gt; WriteQueue host2 -\u0026gt; CITI2 {rank=min;host1;host2} }   Tunnel の再接続   tunnel が切断されても、 tunnel を再接続すれば tunnel 内に流れる tcp セッションは継続通信可能である。  ただし、tcp 通信のタイムアウト以内に再接続できることが条件である。  tunnel を再接続すれば tcp セッションは継続通信可能だ。 しかし、そう単純にはいかないケースがある。 それは『送信したつもりになっているパケットが、相手に届いていないことがある』からだ。 この場合、相手に届いていないパケットを送信しなおす必要がある。  「tcp は udp と違って再送制御などを行なって信頼性を確保しているんじゃないのか？」 と思う人もいるだろう。私も最初はそう思っていた。 しかし、実際はそうではない。 なぜなら、再送制御などはあくまでも TCP セッションが続いている場合に行なわれることで、 TCP セッションが切断された場合は再送制御なども当然破棄される。  つまり、強制的にセッションが切断された場合は、 送ったつもりのデータが相手に届いていないことが普通にありえる。  このような「送ったつもりが相手に届いていないデータ」がある場合、 TCP セッションを継続させるにはそのデータを再送してやる必要がある。 この再送処理は、 packetWriter スレッドが実行する。 フロー制御   前述の通り、再接続後は送信側と受信側とでデータの不整合を確認し、 受信されていないデータの再送信が必要になる。  これを実現するには、送信済みデータを保持しておく必要がある。 しかし、全ての送信済みのデータを保持しておく訳にもいかないので、 保持可能なパケット数を決めておく。 そして保持可能なパケット数と相手が受信していないパケット数のバランスが 崩れないようにフロー制御を行なう。  もっとも単純なのは、送信するたびに相手の受信を持ってから次の送信を行なうことだが、 これだと通信効率が悪すぎる。 そこで、保持可能なパケット数の半分づつ確認を行なっている。 participant stream2Tunnel_client participant packetReader_client participant packetWriter_client participant packetWriter_server participant packetReader_server participant tunnel2Stream_server stream2Tunnel_client -\u0026gt; stream2Tunnel_client : check the count send packets. stream2Tunnel_client -\u0026gt;\u0026gt; packetWriter_client : write the packet to client queue packetWriter_client -\u0026gt;\u0026gt; packetReader_server : write the packet packetReader_server -\u0026gt;\u0026gt; tunnel2Stream_server : read the packet to server queue tunnel2Stream_server -\u0026gt; tunnel2Stream_server : count received packets. tunnel2Stream_server -\u0026gt;\u0026gt; packetWriter_server : write the sync to server queue packetWriter_server -\u0026gt;\u0026gt; packetReader_client : write the sync      stream2Tunnel は、パケットを queue に書き込む前に送信済みパケット数を確認する。    保持可能なパケット数の半分であれば、 sync を待つ      tunnel2Stream は、受信したパケット数をカウントし、 保持可能なパケット数の半分であれば sync を queue に入れる   リングバッファ   前述の通り再送信のデータ保持のためにフロー制御を行なっている。 このデータ保持用のバッファは、 保持可能なパケット数分のバッファを通信開始時に用意しておき、 それをリングバッファにして使い回している。 digraph G { rankdir = TB; node0 [shape=rect; label = \u0026#34;buf\u0026#34;] node1 [shape=rect; label = \u0026#34;buf\u0026#34;] node2 [shape=rect; label = \u0026#34;buf\u0026#34;] node3 [shape=rect; label = \u0026#34;buf\u0026#34;] node4 [shape=rect; label = \u0026#34;buf\u0026#34;] node5 [shape=rect; label = \u0026#34;buf\u0026#34;] node0 -\u0026gt; node1 node1 -\u0026gt; node2 node2 -\u0026gt; node3 node3 -\u0026gt; node4 node4 -\u0026gt; node5 node5 -\u0026gt; node0 {rank=same; node1;node5} {rank=same; node2;node4} }   送信パケットの結合   tunnel は 2 つの Host の間のパケットを中継する。 一つのパケットは、MTU サイズに近いほど効率よく送信することができる。  そこで、細かいパケットを 1 つのパケットに結合して送信する処理を行なう。  次の図で示す通り tunnel に送信するパケットは stream2Tunnel から WriteQueue に入れられる。 そして packetWriter でパケットを取り出して tunnel に送信する。   この packetWriter でパケットを取り出す時に、 WriteQueue に複数のパケットが入っている場合、 そのパケットを結合して送信する。  packetWriter は、パケットを結合するために積極的にパケットが溜るのを待つことはない。 よって、通信のリアルタイム性が損なわれることはない。 protocol   ここでは tunnel client/server 間で通信を開始する時の protocol について説明する。  protocol は 3 つの情報をやり取りする。 participant server participant client server -\u0026gt;\u0026gt; client : AuthCallenge server \u0026lt;\u0026lt;- client : AuthResponse server -\u0026gt;\u0026gt; client : AuthResult    この protocol の後は、-port オプションで指定されたポートをリスニングし、 アクセス毎に TCP 接続セッションを開始する。 AuthCallenge   AuthCallenge は、次の情報を client に通知する。    Challenge/Response 認証の Challenge 情報    バージョン    サーバの動作モード    client は、この情報から Challenge/Response の Response 情報を生成する。 AuthResponse   AuthResponse は、次の情報を server に通知する。    Challenge/Response 認証の Response 情報    セッションID    新規接続か、切断時の再接続かを示す。    新規の場合 0。再接続の場合、再接続先を示すセッションID。      client 側パケットの WriteNo/ReadNo    再接続する時、再送信が必要かどうかを確認するためのパケット情報      制御コード    特殊な処理を行なう場合に指定する。    例えば tunnel 間のラウンドトリップタイムを計測するモードを指定できる。      server は、この情報から client 認証を行なう。 AuthResult   AuthResult は、次の情報を client に通知する。    認証結果    セッションID    どのセッション ID を使用して通信を行なうかを示す。      Server 側パケットの WriteNo/ReadNo    以上で、 tunnel の client/server 間の接続が確立する。 開発言語   この Tunnel ツールの開発には、次の技術が不可欠である。    TCP    Proxy Client    HTTP Client/Server    WebSocket Client/Server    これら技術との相性の良さという意味では、 node.js が一番始めに候補に上りそうな気がする。 しかし、今は Go の勉強中ということもあり Go で開発を行なった。 ","id":5,"section":"posts","summary":"とある理由から 「Tunnel/Reverse Tunnel over websocket」 が必要になったので作ってみた。 「Tunnel/Reverse Tunnel over webs","tags":null,"title":"Tunnel/Reverse Tunnel over websocket を作った","uri":"https://ifritjp.github.io/blog2/public/posts/2020/2020-05-29-tunnel/","year":"2020"},{"content":" 技術情報を GitHub Pages で公開するにあたって、 Hugo を使うことにした。  Hugo は Markdown で静的サイトを構築するツールだが、org-mode にも対応している。 「対応」といっても、当然完全なものではない。  今回 Hugo を org-mode で使ってハマった点を紹介する。  *「TITLE は文書の先頭に書く」  hugo で使用する .org のファイルは、先頭に TITLE を書かなければならない。  .org に記載されている #+TITLE 自体は認識しているようなのだが、 それが先頭に無い限りその記事のタイトルとしては認識されない。  例えば、 emacs では次のように文書内に coding 等を指定することは良くあると思うが、 こうすると Hugo は TITLE を認識してくれない。 # -*- coding:utf-8 -*- #+TITLE: Hugo を org-mode で使う時の注意点   これが判明するまでに、1 時間以上掛ったよ。。 ","id":6,"section":"posts","summary":"技術情報を GitHub Pages で公開するにあたって、 Hugo を使うことにした。 Hugo は Markdown で静的サイトを構築するツールだが、org-mode にも対応している。 「対応」と","tags":null,"title":"Hugo を org-mode で使う時の注意点","uri":"https://ifritjp.github.io/blog2/public/posts/2020/2020-03-29-hugo-org/","year":"2020"},{"content":"  raspberry pi に SSD を接続して簡易 NAS にしている。 この簡易 NAS では、 SSD を取り外ししやすいように autofs によるマウントを設定した。 しかし、SSD を接続すると PCManFM の自動マウントが動いて autofs が正常にマウントできない現象が発生した。  そこで PCManFM の自動マウントを無効化した。 PCManFM の自動マウントを無効化   ~/.config/pcmanfm/LXDE-pi/pcmanfm.conf の以下の設定を変更する。 mount_on_startup=0 mount_removable=0   これで、PCManFM の自動マウントを無効化できる。 ","id":7,"section":"posts","summary":"raspberry pi に SSD を接続して簡易 NAS にしている。 この簡易 NAS では、 SSD を取り外ししやすいように autofs によるマウントを設定した。 しかし、SSD を接続すると PCManFM の自動","tags":null,"title":"raspberry pi の USB MASS STORAGE 自動マウントを無効化する","uri":"https://ifritjp.github.io/blog2/public/posts/2020/2020-02-21-raspberrypi-mount/","year":"2020"},{"content":" emacs lisp の quote でハマったのでネタに書いておく。 (defvar hoge-val nil) (defun hoge-init () (setq hoge-val \u0026#39;(:val nil)) ) (defun hoge-set () (plist-put hoge-val :val \u0026#34;1\u0026#34;))   上記のように変数 hoge-val に対して plist-put で処理する関数を定義して、 次のようにコールすると。 (let (val1 val2 val3) (hoge-init) (setq val1 (plist-get hoge-val :val)) (hoge-set) (setq val2 (plist-get hoge-val :val)) (hoge-init) (setq val3 (plist-get hoge-val :val)) (message (format \u0026#34;%s %s %s\u0026#34; val1 val2 val3)))   最後の (message (format \u0026#34;%s %s %s\u0026#34; val1 val2 val3)) で \u0026#34;nil 1 1\u0026#34; が出力される。  てっきり、 \u0026#34;nil 1 nil\u0026#34; が出力されるものだと思っていた。 なぜなら、val3 をセットする直前に hoge-init を実行しており、 この hoge-init は hoge-val を \u0026#39;(:val nil) で初期化する関数なので、 (plist-get hoge-val :val) は nil を返すと考えたからだ。  しかし実際には、最後の (plist-get hoge-val :val) は \u0026#34;1\u0026#34; になる。  なぜこのような結果になるかと言うと、 \u0026#39;() は定数として扱い、 関数 hoge-init を実行する際には新しくリストを生成せず、 defun を評価した時の値そのものが使い続けられる。  そして (plist-put) でリストの中身を操作した場合、その定数自体が書き変わり、 hoge-init 関数は変数に書き変わった定数を代入しているため初期化できない。  一方で、 hoge-init の処理に list 関数を使うと、\u0026#34;nil 1 nil\u0026#34; となる。 (defun hoge-init () (setq hoge-val (list :val nil)) )   (list) は評価されるたび新規にリストを生成しているため、変数を初期化出来る。  よく考えてみると納得できるけど、 実際の動きと見た目のギャップにどうにもこうにも意味不明だった。  これまで一度も意識せずにきたのが不思議なくらい、かなり基本的な内容だと思う。  quote した値の変更は、要注意ってことで。 ","id":8,"section":"posts","summary":"emacs lisp の quote でハマったのでネタに書いておく。 (defvar hoge-val nil) (defun hoge-init () (setq hoge-val \u0026#39;(:val nil)) ) (defun hoge-set () (plist-put hoge-val :val \u0026#34;1\u0026#34;)) 上記のように変数 hoge-val に対して plist-put で処理する関数を定義して、 次のように","tags":null,"title":"emacs lisp の quote","uri":"https://ifritjp.github.io/blog2/public/posts/2020/2020-02-18-emacs-quoted-list/","year":"2020"},{"content":"  emacs の org-mode では、 .org ファイル内に C や python 等ソースコードを書いて、 export 時にそのソースコードを色付けした状態で載せることができる。  この機能を babel と言う。  babel では、ソースコードの色付けだけでなく、 dot や plantuml 等のグラフ生成言語を利用することで、 .org ファイル内に書いたグラフ生成言語からグラフを生成して、 所定位置にグラフを挿入することもできる。  今回、 org-mode 9.3.5 の babel を使って dot の画像を出力しようとしたところ、 エラーしたので原因を追ってみた。 エラー箇所   エラーは次の関数で発生していた。 (defun org-babel-chomp (string \u0026amp;optional regexp) \u0026#34;Strip a trailing space or carriage return from STRING. The default regexp used is \\\u0026#34;[ \\\\f\\\\t\\\\n\\\\r\\\\v]\\\u0026#34; but another one can be specified as the REGEXP argument.\u0026#34; (let ((regexp (or regexp \u0026#34;[ \\f\\t\\n\\r\\v]\u0026#34;))) (while (and (\u0026gt; (length string) 0) (string-match regexp (substring string -1))) (setq string (substring string 0 -1))) string))   エラーの内容は次のものだった。 Debugger entered--Lisp error: (wrong-type-argument stringp nil) string-match(nil \u0026#34;c\u0026#34;) (and (\u0026gt; (length string) 0) (string-match regexp (substring string -1))) (while (and (\u0026gt; (length string) 0) (string-match regexp (substring string -1))) (setq string (substring string 0 -1))) (let ((regexp (or regexp \u0026#34;[ \\f\\011\\n\\015\\013]\u0026#34;))) (while (and (\u0026gt; (length string) 0) (string-match regexp (substring string -1))) (setq string (substring string 0 -1))) string)   このエラーは、 上記の org-babel-chomp 関数の regexp 引数が nil だった場合に発生する。 エラーの修正   このエラーに対し、 次のように let で宣言する変数を別名(regexp-work)で定義することで回避した。 (defun org-babel-chomp (string \u0026amp;optional regexp) \u0026#34;Strip a trailing space or carriage return from STRING. The default regexp used is \\\u0026#34;[ \\\\f\\\\t\\\\n\\\\r\\\\v]\\\u0026#34; but another one can be specified as the REGEXP argument.\u0026#34; (let ((regexp-work (or regexp \u0026#34;[ \\f\\t\\n\\r\\v]\u0026#34;))) (while (and (\u0026gt; (length string) 0) (string-match regexp-work (substring string -1))) (setq string (substring string 0 -1))) string))  エラーの原因   エラーの原因を確認するため、 エラーを再現する処理を抜き出して書き換えると次になる。 ;;; -*- lexical-binding: t; -*- (defun hoge (regexp) (let ((regexp (or regexp \u0026#34;a\u0026#34;))) (string-match regexp \u0026#34;b\u0026#34;)))   上記の hoge 関数の引数 regexp に nil をセットしてコールすると同じエラーになる。 なお、この現象は lexical-binding を有効にしている時だけ発生する。  上記関数の処理を説明すると次のようになる。    let で新しく変数 regexp を宣言する    このとき、引数 regexp が nil 以外なら、引数 regexp の値を変数 regexp にセットする    引数 regexp が nil なら、 \u0026#34;a\u0026#34; を変数 regexp にセットする。      つまり、let で宣言している変数 regexp には必ず nil 以外がセットされるはずである。  しかし、実際には string-match に渡される regexp には nil がセットされている。  何故このような結果になるか原因を想像すると、  「string-match でアクセスするシンボル regexp は、 let で宣言している regexp ではなく、関数の引数 regexp が参照されるため」  と考えるのが妥当だろう。  string-match は let のスコープなので、 普通に考えれば string-match の regexp は let で宣言している変数 regexp であるはず。 しかし、実際には何故か関数の引数 regexp になっている。  これが emacs lisp の仕様なのか、はたまた仕様外の動作なのかは良く分からない。  ちなみに、これが発生している環境は emacs 26.2 だが、 他の環境で発生するかどうかは確認していない。  org-mode の履歴を追ってみたが、 この関数の処理は lexical-binding を使うようになる前から変っていないので、 lexical-binding にした事による影響だろう。  以上。 ","id":9,"section":"posts","summary":"emacs の org-mode では、 .org ファイル内に C や python 等ソースコードを書いて、 export 時にそのソースコードを色付けした状態で載せることができる。 この機能を babel と言う。 babel で","tags":null,"title":"org-mode 9.3.5 で babel(dot/plantuml) が動かなかった","uri":"https://ifritjp.github.io/blog2/public/posts/2020/2020-02-13-emacs-org-9.3.5/","year":"2020"},{"content":"  この記事は、emacs 用 reviewboard モードの宣伝である。  \u0026lt;https://github.com/ifritJP/emacs-reviewboard-front\u0026gt;  reviewboard は、ソースコードレビューを Web 上で行ない記録するためのツール。  今は github の Pull-Request に代表されるように Web 上のソースレビューが普及しているが、 reviewboard の初版が 2007 年であることを考えると、 当時は先進的なツールだったと思う。  そんな reviewboard を emacs で操作するモードを今になって作ったので、 どれ程の人が使うかは不明だが、折角なので宣伝しておく。 機能   このモードでは、次の機能を提供する。    修正ファイル一覧から必要なファイルを選択して review request (以降 rrq と記す)を登録    rrq の summary/description/testing_done を編集    修正ファイルの追加、削除可能      レビューを受けて更新したファイル郡を、一発でアップロード    レビューコメントのリプライ登録    rrq の publish/close/discard    rrq に登録したファイルをコミット   設定  環境     curl, rbt, svn を事前にインストールしておく    rbt は、 diff の登録に利用する。    curl は、 reviewboard の WebAPI へのアクセスに利用する。    環境によっては、 proxy 等の環境変数設定が必要な場合がある。      上記 github から emacs-reviewboard-front を取得し適当な場所に展開する   emacs-lisp     emacs-reviewboard-front のパスに load-path に追加する。    次の設定を行なう。   (require \u0026#39;rbfront-mode) (setq rb/front-rb-api-token \u0026#34;TOKEN\u0026#34;) (setq rb/front-rb-url \u0026#34;http://reviewboard.host/path\u0026#34;) (setq rb/front-rbt \u0026#34;rbt\u0026#34;) (setq rb/front-proxy \u0026#34;http://proxy.host:8080/\u0026#34;) (setq rb/front-rb-repository \u0026#34;RESPOSITORY_NAME\u0026#34;)     rb/front-rb-api-token は、 reviewboard のアカウント管理ページで生成した API Tokens を指定する。    rb/front-rb-url は、 reviewboard のサーバの URL を指定する。    rb/front-proxy は、 reviewboard のサーバにアクセスする際に使用する proxy を指定する。    front-rb-repository は、 reviewboard に diff を登録する際の repository 名を指定する。   新規登録   emacs-reviewboard-front では、現状 svnp.el を使用することを前提としている。  ここでは、svnp.el の細かい使用方法については説明しない。 rrq の新規登録に必要な最低限の操作について説明する。    M-x svn-status で修正ファイル一覧を表示し、 commit する要領でファイルを選択する。    j キー押下で、rrq 編集バッファが表示される。   編集バッファ      title と description、 test を編集する。    編集後、 C-c C-c 押下により submit 処理で reviewboard に登録する。    新規登録の場合、 mini-buffer で reviewer を選択する。    この mini-buffer では TAB キーによる補完が可能。   修正ファイルの追加・削除       rrq に登録する修正ファイルを追加したい場合、 C-c C-a を押下する。    mini-buffer で、ファイルが存在するディレクトリを指定し、 その後表示されるファイル一覧から上記のようにファイルを選択する。    選択後、 j キー押下で、ファイルが追加される。      rrq に登録する修正ファイルを除外する場合、 除外するファイルにカーソルを移動して C-c C-SPC を押下する。    除外を reviewboard に反映するには、 C-c C-u を押下する。 review コメント       review コメントの表示はサーバアクセスが多くなるため、 デフォルトでは非表示にしている。    表示する場合、 C-c C-d する。    デフォルトで表示にする場合、 rb/front-display-comment-p に nil 以外を設定する。    review コメントに対するリプライを登録する場合、 コメントにカーソルを合わせて C-c C-r。 submit モード     submit 時の動作を、次のどちらかに変更できる。    submit と同時に publish する    submit だけする    C-c C-t でモードを切り替える。  デフォルトは publish する。  デフォルトを submit だけに切り替える場合、 rb/front-submit-and-publish-p に nil を設定する。 rrq リスト表示   M-x rb/front-list で、 自分が登録した rrq 一覧を表示する。  リスト操作    (g)    リストを更新する   (RET)    カーソル位置の rrq を編集する   (u)    カーソル位置の rrq の diff を、再アップロードする   (p)    カーソル位置の rrq を publish する。   (c)    カーソル位置の rrq を close する。   (d)    カーソル位置の rrq を discard する。   (C)    カーソル位置の rrq に登録したファイルを commit する。   diff の再アップロード   再アップロードを行なうため、ローカルの work ディレクトリを指定する必要がある。 work ディレクトリの指定は mini-buffer で行なう。 注意     rrq 編集バッファで C-c C-c を実行すると、 バッファ内容がサーバに登録され、即時 publish する。    rrq 編集バッファの C-c C-a による修正ファイル追加は、 新規 rrq の場合を除き即時 publish する。 新規 rrq の場合、submit 時に rrq 情報と一緒に更新ファイル情報が登録される。   ","id":10,"section":"posts","summary":"この記事は、emacs 用 reviewboard モードの宣伝である。 \u0026lt;https://github.com/ifritJP/emacs-reviewboard-front\u0026gt; reviewboard は、ソースコードレビューを Web 上で行ない記録するためのツール。 今は github の Pull-Request に代表されるように Web","tags":null,"title":"emacs 用 reviewboard モードの宣伝","uri":"https://ifritjp.github.io/blog2/public/posts/2020/2020-02-03-emacs-reviewboard/","year":"2020"},{"content":" プログラムを組む際、ラッパー関数を作ることは良くある。  このラッパー関数のオーバーヘッドが気になったので簡単に調べてみた。  計測用サンプルは次の通り。 #include\u0026lt;stdio.h\u0026gt;typedef void (func_t)( int val1, int val2 ); void func( int val1, int val2 ) { printf( \u0026#34;%d %d\u0026#34;, val1, val2 ); } void wrapper0( int val1, int val2 ) { func( val1, val2 ); } void wrapper1( func_t * pFunc, int val1, int val2 ) { pFunc( val1, val2 ); } void wrapper2( int val1, int val2, func_t * pFunc ) { pFunc( val1, val2 ); } main() { wrapper0( 0, 1 ); wrapper1( func, 0, 1 ); wrapper2( 0, 1, func ); }   関数 func() をコールする 3 種類のラッパー関数 wrapper0, wrapper1, wrapper2 を用意した。  それぞれのラッパー関数は次の形になっている。    ラッパー 引数     wrapper0 呼び出し先と同じ引数   wrapper1 ラッパー独自引数の後に呼び出し先と同じ引数   wrapper2 呼び出し先と同じ引数の後にラッパー独自引数     これを gcc の x64 で -O の最適化した結果が次になる。 (func の処理は省略) 0000000000000021 \u0026lt;wrapper0\u0026gt;: 21:\t48 83 ec 08 sub $0x8,%rsp 25:\te8 00 00 00 00 callq 2a \u0026lt;wrapper0+0x9\u0026gt; 2a:\t48 83 c4 08 add $0x8,%rsp 2e:\tc3 retq 000000000000002f \u0026lt;wrapper1\u0026gt;: 2f:\t48 83 ec 08 sub $0x8,%rsp 33:\t48 89 f8 mov %rdi,%rax 36:\t89 f7 mov %esi,%edi 38:\t89 d6 mov %edx,%esi 3a:\tff d0 callq *%rax 3c:\t48 83 c4 08 add $0x8,%rsp 40:\tc3 retq 0000000000000041 \u0026lt;wrapper2\u0026gt;: 41:\t48 83 ec 08 sub $0x8,%rsp 45:\tff d2 callq *%rdx 47:\t48 83 c4 08 add $0x8,%rsp 4b:\tc3 retq 000000000000004c \u0026lt;main\u0026gt;: 4c:\t48 83 ec 08 sub $0x8,%rsp 50:\tbe 01 00 00 00 mov $0x1,%esi 55:\tbf 00 00 00 00 mov $0x0,%edi 5a:\te8 00 00 00 00 callq 5f \u0026lt;main+0x13\u0026gt; 5f:\tba 01 00 00 00 mov $0x1,%edx 64:\tbe 00 00 00 00 mov $0x0,%esi 69:\tbf 00 00 00 00 mov $0x0,%edi 6e:\te8 00 00 00 00 callq 73 \u0026lt;main+0x27\u0026gt; 73:\tba 00 00 00 00 mov $0x0,%edx 78:\tbe 01 00 00 00 mov $0x1,%esi 7d:\tbf 00 00 00 00 mov $0x0,%edi 82:\te8 00 00 00 00 callq 87 \u0026lt;main+0x3b\u0026gt; 87:\tb8 00 00 00 00 mov $0x0,%eax 8c:\t48 83 c4 08 add $0x8,%rsp 90:\tc3 retq   上記通り wrapper0 と wrapper2 は、ほぼ同じコードになっており、 wrapper1 は引数をずらす処理が余分に入っている。  想像通りの結果といえば想像通りだが、 ちゃんと最適化された処理になっている。  以上のことから言えることは、 ラッパー関数独自の引数は、先頭ではなく末尾にもっていった方が良いということだ。  ただし、ここまで最適化が効くケースは、 ラッパー関数内での目的の関数コールが先頭にある場合に限られるので、 目的の関数コールを先頭に持ってこれない場合は、気にしないで良いだろう。  なお、 -O2 で最適化をかけると wrapper1, wrapper2 は次の処理に最適化された。 0000000000000030 \u0026lt;wrapper1\u0026gt;: 30:\t48 89 f8 mov %rdi,%rax 33:\t89 f7 mov %esi,%edi 35:\t89 d6 mov %edx,%esi 37:\tff e0 jmpq *%rax 39:\t0f 1f 80 00 00 00 00 nopl 0x0(%rax) 0000000000000040 \u0026lt;wrapper2\u0026gt;: 40:\tff e2 jmpq *%rdx   個人的には、こっちの方が納得がいく。  また、次のようにラッパー関数に static 宣言を付加して、 外部からコールされないことを明示すると、 #include\u0026lt;stdio.h\u0026gt;typedef void (func_t)( int val1, int val2 ); void func( int val1, int val2 ) { printf( \u0026#34;%d %d\u0026#34;, val1, val2 ); } static void wrapper0( int val1, int val2 ) { func( val1, val2 ); } static void wrapper1( func_t * pFunc, int val1, int val2 ) { pFunc( val1, val2 ); } static void wrapper2( int val1, int val2, func_t * pFunc ) { pFunc( val1, val2 ); } main() { wrapper0( 0, 1 ); wrapper1( func, 0, 1 ); wrapper2( 0, 1, func ); }   出力結果は次のように、 ラッパーがインライン展開され、 ラッパーの引数の違いによる差分は無くなった。 0000000000000021 \u0026lt;main\u0026gt;: 21:\t48 83 ec 08 sub $0x8,%rsp 25:\tbe 01 00 00 00 mov $0x1,%esi 2a:\tbf 00 00 00 00 mov $0x0,%edi 2f:\te8 00 00 00 00 callq 34 \u0026lt;main+0x13\u0026gt; 34:\tbe 01 00 00 00 mov $0x1,%esi 39:\tbf 00 00 00 00 mov $0x0,%edi 3e:\te8 00 00 00 00 callq 43 \u0026lt;main+0x22\u0026gt; 43:\tbe 01 00 00 00 mov $0x1,%esi 48:\tbf 00 00 00 00 mov $0x0,%edi 4d:\te8 00 00 00 00 callq 52 \u0026lt;main+0x31\u0026gt; 52:\tb8 00 00 00 00 mov $0x0,%eax 57:\t48 83 c4 08 add $0x8,%rsp 5b:\tc3 retq   基本的に、ソースコードはメンテナンス性や可読性を優先すべきだが、 ソースコードを自動生成するような場合は、 このような細かいことも意識しておいた方が良いだろう。  以上。 ","id":11,"section":"posts","summary":"プログラムを組む際、ラッパー関数を作ることは良くある。 このラッパー関数のオーバーヘッドが気になったので簡単に調べてみた。 計測用サンプルは次の","tags":null,"title":"C 言語のラッパー関数オーバーヘッド","uri":"https://ifritjp.github.io/blog2/public/posts/2019/2019-10-15-wrapper-overhead/","year":"2019"},{"content":"  以前 C 言語の関数ポインタによる関数コールのオーバーヘッドがどの程度なのか調べたが、 今回は可変長引数(va_list)処理のオーバーヘッドについて調べてみた。 結果   初めに結果から書くと、 可変長引数(va_list)処理のオーバーヘッドは、めちゃめちゃ掛る。 また、引数の数に応じて時間が増加する。  所感   今回の実験によって、 va_list 処理には当初の想定を遥かに越えたオーバーヘッドが かかることが分った。  個人的には、コンパイラがもっと賢くやってくれているものだと思っていたが、 実際には全く賢くなかった。  C 言語で可変長引数を積極的に使用することはあまりないとは思うが、 可変長引数の使用はオーバーヘッドを十分考慮に入れて慎重に検討するべきだということが判った。  この可変長引数のオーバーヘッドを調べたのは、 LuneScript のメソッド呼び出し処理を C 言語にトランスコンパイルした際に 可変長引数を利用しようと思ったからなのだが、 この結果から可変長引数は使えないことが分った。  対応する前に結果が分って良かったが、 可変長引数が使えなくなったのは当初の目論見が崩れてしまった。 実験詳細   ここでは、今回の実験方法について説明する。 コード   実験用に次の C 言語コードを作成した。 int func( int val1, int val2 ) { return val1 + val2; } int sub( int dummy, int val1, int val2 ) { return func( val1, val2 ); } int funcv2( va_list ap ) { int val1 = va_arg( ap, int ); int val2 = va_arg( ap, int ); return val1 + val2; } int subv2( int dummy, ... ) { int val; va_list ap; va_start( ap, dummy ); val = funcv2( ap ); va_end( ap ); return val; }   func, sub は、可変長引数を使用しないパターン。 funcv2, subv2 は、可変長引数を使用しするパターン。  ちなみにコードの全体は次の通りである。 #include \u0026lt;sys/time.h\u0026gt;#include \u0026lt;time.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdarg.h\u0026gt; int func( int val1, int val2 ) { return val1 + val2; } int sub( int dummy, int val1, int val2 ) { return func( val1, val2 ); } int funcv2( va_list ap ) { int val1 = va_arg( ap, int ); int val2 = va_arg( ap, int ); return val1 + val2; } int subv2( int dummy, ... ) { int val; va_list ap; va_start( ap, dummy ); val = funcv2( ap ); va_end( ap ); return val; } int funcv3( va_list ap ) { int val1 = va_arg( ap, int ); int val2 = va_arg( ap, int ); int val3 = va_arg( ap, int ); return val1 + val2 + val3; } int subv3( int dummy, ... ) { int val; va_list ap; va_start( ap, dummy ); val = funcv3( ap ); va_end( ap ); return val; } double getTime( void ) { struct timeval tm; gettimeofday( \u0026amp;tm, NULL ); return tm.tv_sec + tm.tv_usec / 1000000.0; } main( int argc, const char * argv[] ) { long long loop = strtoll( argv[ 1 ], NULL, 10 ) * 1000ll; long long count = 0; int sum = 0; double prev = getTime(); if ( strcmp( argv[ 2 ], \u0026#34;1\u0026#34; ) == 0 ) { for ( count = 0; count \u0026lt; loop; count++ ) { sum += sub( 0, 1, 2 ); } } else if ( strcmp( argv[ 2 ], \u0026#34;2\u0026#34; ) == 0 ) { for ( count = 0; count \u0026lt; loop; count++ ) { sum += subv2( 0, 1, 2 ); } } else { for ( count = 0; count \u0026lt; loop; count++ ) { sum += subv3( 0, 1, 2, 3 ); } } printf( \u0026#34;%s: %lld time = %g, %d\\n\u0026#34;, argv[ 2 ], loop, getTime() - prev, sum ); }   このプログラムは、コマンドラインの引数によって sub, subv2, subv3 を指定の回数分実行し、実行時間を表示する。 計測結果      時間(秒)     固定長引数(sub: 2 引数) 0.62   可変長引数(subv2: 2 引数) 11.95   可変長引数(subv3: 3 引数) 16.16     上記結果を見ると分かる通り、可変長引数は処理時間の桁が違う。  また、引数の数に応じて時間が増加する。  以上 ","id":12,"section":"posts","summary":"以前 C 言語の関数ポインタによる関数コールのオーバーヘッドがどの程度なのか調べたが、 今回は可変長引数(va_list)処理のオーバーヘッドにつ","tags":null,"title":"C 言語の可変長引数 (va_list) 処理のオーバーヘッド","uri":"https://ifritjp.github.io/blog2/public/posts/2019/2019-08-06-va-performance/","year":"2019"},{"content":"  「日本の全てのソフトウェアプロジェクトは必ず技術的負債になる」というタイトルですが、 次の条件を満す場合に限ります。  「プロジェクトに関わるソフトウェアエンジニアの大半が技術に無関心」 動機   このネタは、次の記事を読んで個人的に思うことがあったのをきっかけに 書いています。    オブジェクト指向プログラミング – 1兆ドル規模の大失敗    \u0026lt;https://okuranagaimo.blogspot.com/2019/07/1.html\u0026gt;      大企業の技術系インターンシップに参加した    \u0026lt;https://blog.browniealice.net/post/internship2019winter/\u0026gt;      ソフト開発で世界と闘った及川卓也氏が見た、日本の弱点と可能性    \u0026lt;https://headlines.yahoo.co.jp/article?a=20190801-00010000-chuokou-bus_all\u0026gt;      上記の記事は各自に読んでもらうとして、 それぞれの記事の内容をものすごく大雑把にまとめると    「OOP はダメだから、関数型プログラミングを使え」    「日本を代表する大企業に実情に失望した」    「日本の企業はソフトウェア開発を理解していない」    になると思います。 プログラミング言語は道具にすぎない   上記ブログで「OOP はダメだから、関数型プログラミングを使え」と書かれています。 私は、OOP が万能だなんて思ってませんし、 上記ブログで指摘されている側面があることも理解しています。  ですが、オブジェクト指向プログラミングにしろ関数型プログラミングにしろ、 万能ではないという意味ではどちらも同じです。  プログラム言語は道具です。 いかなる道具であっても、 その道具を安全に運用できるかどうかは、最終的には使う人に依存することになります。  例えば、古典的なプログラミング言語の代表格に C 言語がありますが、 ご存知の通り C 言語には GC もないですし、 NULL 安全でもありません。 そのような高度な「安全」機能を持たない C 言語は、Linux Kernel の開発言語です。 C 言語によって Linux Kernel を開発しているという事実は、 高度な「安全」機能が搭載されていないプログラミング言語であっても、 使用する人次第で大規模プロジェクトでも問題なく運用できるという一つの実証と言えます。  逆に、C 言語よりも高度な「安全」機能を搭載しているプログラミング言語を使用した プロジェクトが技術的負債の塊になり運用困難になった、 という例はいくらでも身近にあると思います。 もし身近に無いとしても、ネットで検索すれば多数ヒットします。  だからと言って、 C 言語の様に使用する人への依存が高過ぎる言語と、 Rust のように先進的な安全機能搭載言語のどちらを使っても大差はない、 というつもりはありません。 私が言いたいのは、「より安全」と言われる技術を使っても、 それを使用する人への依存性が無くなることはない、ということです。  自動運転に例えると、 プログラム言語自体が提供する「安全」は高々レベル 3 のサポートにすぎません。  レベル 3 の自動運転には、ドライバーの運転技術が必須であるように、 現存するどのようなプログラム言語であっても、 ソフトウェアエンジニアの能力が欠かせません。 「ソフトウェアエンジニアの大半が技術に無関心」であることの問題   なぜ「プロジェクトに関わるソフトウェアエンジニアの大半が技術に無関心」だとダメなのか？  これは単純に、そのようなプロジェクトではどのように「安全」な環境であっても、 その「安全」がレベル 5 の自動運転のように「完全」でない限り、 「不具合をエンジニア自ら作り込んでしまう」からです。  前述している通り、プログラミング言語はあくまでも道具であって、 その道具を安全に運用できるかどうかは、最終的には使う人に依存することになります。 そしてプログラミング言語を使う人はソフトウェアエンジニアであり、 ソフトウェアエンジニアの能力は、多くの場合、技術への関心度に比例します。  特に統計を取った訳ではなく、裏付け資料がある訳でもないですが、 個人的な経験上、技術への関心度が高いソフトウェアエンジニアほど能力が高く、 技術への関心度が低いソフトウェアエンジニアほど能力が低い傾向にあります。  つまり、 「プロジェクトに関わるソフトウェアエンジニアの大半が技術に無関心」であるということは、 ソフトウェアエンジニアの大半の能力が低いということと、ほぼ同義になります。  「技術への関心度が低いソフトウェアエンジニアほど能力が低い傾向にある」という持論の 根拠となるエピソードを一つ挙げておきます。  あるソフトウェアエンジニアＡがモジュールの設計をしていました。  そのモジュールは、他モジュールとの依存が高いことが問題になっていたので、 「DI(Dependency injection)の手法を取り入れたら もっとスッキリした設計になる可能性があるので検討してみてはどうですか？」 と、そのソフトウェアエンジニアＡに話をすると、 「そういう難しいことは逆に不具合につながるのでやりたくない」と 言われて一蹴されました。 DI を検討した結果、従来通りの方法を採用する方が良いという結論になったのであれば 納得できますが、なんとなく難しそうというイメージだけで拒否していました。 そして、そのモジュールは依存が高いまま実装されました。  DI のことを理解していれば、それが難しいと考える人はほとんどいないでしょうし、 テストがしやすいことから、むしろ不具合も低減できる可能性があり、 DI を取り入れることで不具合に繋がることを心配する人はいないでしょう。  このように、技術への関心度が低いと、 自分が知らない技術を積極的に取り入れるようなことをせず、 自分が使える技術だけで解決しようとします。 これによって、よりスマートに実現できる方法が他にあるにもかかわらず、 潜在的な問題を含む古い方法によってモジュールが作られていき、 それが積み重なってプロジェクト全体の品質が下っていきます。 そしてそれは時間が経過するほど、手をつけられない技術的負債になります。  一言で表現すれば、技術への関心度が低いエンジニアは「技術的負債製造機」です。  例え TEST FIRST の開発プロセスであっても、それは防げないでしょう。 ならぜなら、 テストというのは作成した成果物が仕様通りに出来ていることを確認するものであって、 仕様そのものに不具合があった場合は、その不具合を検知することは出来ないからです。 仕様を作るのはソフトウェアエンジニアです。 能力の低いソフトウェアエンジニアほど、穴の多い仕様を作る傾向にあります。  能力の低いソフトウェアエンジニアには仕様を作らせず、 能力の高いソフトウェアエンジニアだけで仕様を作れば良い、という考えもあると思います。  確かに、能力の高い人の比率が高い場合はそういう運用が可能かもしれません。 しかし、ここでは大半が能力が低いことを前提にしているので、 そのような運用は難しいです。  また、例え仕様に問題がなくても、 実際にコード化した時に不具合が埋め込まれることは良くあります。 そして、テストで検出されることもなくリリースされ、市場で時限爆弾のように爆発する、 お決まりのパターンです。もはや伝統芸能の域です。 なぜ日本で問題なのか？   ここまでの話を納得していただけたとして、次の疑問が浮ぶかもしれません。  「プロジェクトに関わるソフトウェアエンジニアの大半が技術に無関心」が 技術的負債を生み出す原因ならば、日本でなくても同じことが言えるのではないか？  それは確かにそうです。 しかし、日本の場合、終身雇用 \u0026amp; 転職しずらい社会環境によって、 一度雇ったソフトウェアエンジニアが技術に無関心だったとしても、 そのソフトウェアエンジニアを他の優秀なソフトウェアエンジニアに入れ替える、 ということが非常に困難なため、このような状況になり易いです。  さらに、日本ではソフトウェア開発をゼネコン方式で開発するという文化があり、 一つのプロジェクトを社内の優秀なソフトウェアエンジニアだけで開発する、 というのは非常に稀なケースであり、 一部(あるいは全部)のモジュールをアウトソーシングするケースが多くあります。  これによって、プロジェクトの品質コントロールをより困難にしています。  また、日本では全ての社員の待遇に差を付けず、 等しくすることを善しとする文化があるようで、 ソフトウェアエンジニアの能力に応じた待遇にする、というようなことを滅多にしません。 一方で、マネジメント能力に関しては、 能力に応じた待遇にするキャリアパスが古くから存在するため、 自分ではコードを一切書かないで一日中パワーポイントやエクセルの資料をせっせと作成している ソフトウェアエンジニア(？)が多く存在します。 そして、マネジメント能力以外のソフトウェアエンジニアの能力が評価対象ではないため、 自然と「プロジェクトに関わるソフトウェアエンジニアの大半が技術に無関心」と いう状況になる傾向にあります。 いわゆる Japanese Traditional Big Company では、 特にこの傾向が顕著なのではないでしょうか？  最初に紹介したブログの著者が「日本を代表する大企業に実情に失望した」原因は、 このような背景があるためだと思います。  また、このような背景を作り出しているのは、 Yahoo の記事にある「日本の企業はソフトウェア開発を理解していない」ためだと思います。  以上のように、日本のソフトウェア開発プロジェクトには 技術的負債を生み出す環境が整っているため、 いかなる開発手法、プログラム言語を用いても技術的負債化を防ぐことは出来ません。  それなのに、この状況を改善する為と称して、新しいプロジェクト進捗管理手法を導入する、 という斜め上な施策が実施されることがあります。  どういう論理で考えると、「新しいプロジェクト進捗管理手法を導入すること」と、 「プロジェクトの技術的負債化を防ぐこと」が繋がるのでしょうかね？ 最後に   私は LuneScript という言語を開発しています。 「プログラム言語は単なる道具でしかない」というのは、 ある意味自己否定しているようにも思われるかもしれません。  ですが、プログラム言語自体で提供できる安全機能は まだまだ残っていると思っているので、 ソフトウェアエンジニアの助けになるような安全機能を提供できるように 今後も開発を続けていきたいと考えています。  以上。 ","id":13,"section":"posts","summary":"「日本の全てのソフトウェアプロジェクトは必ず技術的負債になる」というタイトルですが、 次の条件を満す場合に限ります。 「プロジェクトに関わるソフ","tags":null,"title":"如何なる開発手法、プログラム言語を用いても、日本の全てのソフトウェアプロジェクトは必ず技術的負債になる","uri":"https://ifritjp.github.io/blog2/public/posts/2019/2019-08-02-engineering/","year":"2019"},{"content":"  emacs のバージョンを 26.2 に変えたことで、 色々と細かいところの使い勝手が変っている。  その中で、 → 等の一部のフォントが半角表示されるようになったのが 微妙にストレスだったのでちょっと追ってみた。 原因   原因、と言うよりは起因と言った方が良いかもしれないが、 → 等の一部のフォントが半角表示されるようになったのは、 フォントに \u0026#34;DejaVu Sans Mono\u0026#34; を使用していることに起因していた。  これを \u0026#34;Bitstream Vera Sans Mono\u0026#34; に変更することで、現象が治った。  全く同じ環境で、 emacs 26.2 ではなく、以前使用していたバージョンの emacs だと 現象は発生しなかった。  emacs の処理が変ったことが原因であるのはほぼ間違い無いが、 emacs の何がどう変ってこの現象が発生し、 どう設定(使用するフォントを変える以外で)すれば、 現象を修正できたのかは残念ながら分からないまま。  と、思ったが、次のブログに答えがあった。  \u0026lt;http://misohena.jp/blog/2017-09-26-symbol-font-settings-for-emacs25.html\u0026gt;  詳しくは、上記ブログを確認してもらうとして、 要点だけ説明すると use-default-font-for-symbols に nil 以外が設定されていると、 シンボル等の文字のフォントが default フォントを使用するようになるらしい。 このデフォルト値が t であるため、矢印等の一部のフォントが半角になっていた。  ということで、 以下を設定してやれば、使用するフォントを変えなくても全角で表示されるようになる。 (setq use-default-font-for-symbols nil)   じゃぁ、どうして \u0026#34;Bitstream Vera Sans Mono\u0026#34; に変えると 全角で表示されたのか？が気になったんで調べてみたが、 どうやら \u0026#34;Bitstream Vera Sans Mono\u0026#34; には矢印などのフォントが 含まれていなことが原因のようだ。  fontforge でフォントの中身を見ると、 \u0026#34;Bitstream Vera Sans Mono\u0026#34; には矢印のフォントがなく、 \u0026#34;DejaVu Sans Mono\u0026#34; には矢印のフォントがあることが判った。  つまり、\u0026#34;DejaVu Sans Mono\u0026#34; には矢印のフォントがあるので、それが表示され、 \u0026#34;Bitstream Vera Sans Mono\u0026#34; には矢印のフォントがないので、 別で設定していた全角のフォントが表示された、ということだろう。  あぁ、これでストレスが一つ減った。 ","id":14,"section":"posts","summary":"emacs のバージョンを 26.2 に変えたことで、 色々と細かいところの使い勝手が変っている。 その中で、 → 等の一部のフォントが半角表示されるようになったのが 微","tags":null,"title":"emacs26.2 で矢印(→)等の一部のフォントが半角表示されるようになった","uri":"https://ifritjp.github.io/blog2/public/posts/2019/2019-07-19-font/","year":"2019"},{"content":"  これは seekable な stream と none_seekable な stream の使い分けに関する記事です。  使い分けが十分出来ている人は読まなくても大丈夫です。  皆さんは bitstream という単語をご存知でしょうか？  AV (Audio\u0026amp;Visual) が好きな人や、 それらの業界に関係のある人ならそこそこ聞く単語だと思いますが、 一般的にはあまり馴染の無い単語でしょうか。  馴染の無い人の為に身近な HDD レコーダを例に挙げて説明すると、 HDD レコーダはデジタル放送の電波に乗っているデータをそのまま記録していますが、 このデータが bitstream です。 HDD レコーダは、デジタル放送の bitstream を HDD に記録し、 記録した bitstream を再生する装置と言えます。 もちろん、実際にはそんな単純ではないですが、概ね間違ったことは言ってません。 stream   プログラムでデータを扱う時、stream という概念を使って制御します。    言語 stream (入力)     Java InputStream   Swift InputStream   Go io.Reader     上記は言語毎の入力系 stream の例です。  ちなみに入力系の stream とは何かというと、 流れてくるデータを読み出すためのものです。  例えば、先ほどの HDD レコーダの例で説明すると、    デジタル放送の電波に乗っている bitstream を読み取る部分    HDD に記録されている bitstream を読み込む部分    が入力系の stream です。  また、上記言語の stream (InputStream,io.Reader)には共通することがあります。  それは、データの流れが一方通行で遡ることが出来ない、ということです。  プログラム的に言うと、上記の stream は seek や rewind をサポートしていません。  これを、先ほどの HDD レコーダの例で説明すると、 「過去に放送された番組の録画はできない」ということです。  24 時間全ての番組を常に録画し続けて、 「1週間前に放送された任意の番組を再生する」機能を持つ HDD レコーダはありますが、 それはあくまで録画してあるものを再生しているのであって、 過去に放送された番組を録画することは出来ません。 もしそれが出来るなら、 本当の意味でのタイムマシーンを作ることが出来ることと同義になります。  なお、「過去に放送された番組の録画はできない」ですが、 「録画した番組」の逆再生などは出来ます。  先ほど説明した通り、次のどちらもの入力 stream です。    デジタル放送の電波に乗っている bitstream を読み取る部分    過去に放送された番組の録画はできない      HDD に記録されている bitstream を読み込む部分    録画した番組は逆再生など出来る      これはつまり、 stream には次の 2 つのタイプが存在することを意味します。    流れが一方通行で遡ることが出来ない stream    流れを遡ることが出来る stream    これ以降、上記をそれぞれ none_seekable と seekable とします。 none_seekable と seekable の使い分け   上記の通り、stream には none_seekable と seekable の 2 つのタイプが存在します。  では、実際のプログラムでは stream はどう使い分けるべきか？ と考えた場合、 seekable である必要がない場合は極力 none_seekable を使うべきです。  なぜならば、 seekable は none_seekable を包括する概念であり、 seekable な stream は none_seekable として使用することが出来ますが、 none_seekable な stream は seekable として使用することが出来ないからです。  次に、疑似言語を使って説明します。 fn funcA( data: seekable ) { sub( data ); } fn funcB( data: none_seekable ) { sub( data ); }   上記は、 seekable な引数を持つ関数 funcA と、 none_seekable な引数を持つ関数 funcB を定義する疑似言語コードです。 また sub() は、 none_seekable な引数を持つ関数とします。  ここで、この関数 funcA は seekable な stream でしか使用できないのに対し、 この関数 funcB は seekable, none_seekable どちらでも使用できることになり、 funcB は funcA よりも汎用性が高いと言えます。  関数の汎用性が高いことが良いプログラムである、とは一概には言えませんが、 ミドルウェアなどのライブラリでは、汎用性が高い方が良いとされます。  つまり、 stream を入力に持つ関数の処理においては、 seek や rewind の使用は極力避け、 none_seekable の stream で処理可能にすべきである、と言えます。  ただし例外として、 seek や rewind を使用しないと目標のパフォーマンスが出ないとか、 必要なワークメモリが規定を越えてしまう、等の問題がある場合は、 無理に none_seekable で処理する必要はありません。  とはいえ、あくまでも原則は、 seekable ではなく none_seekable で処理できるかどうかを検討するべきです。  言語の組込みの型として seekable と none_seekable が分かれていない言語は、 結構あると思います。  そのような言語でも、 seekable と none_seekable の考え方自体は有効なので実践してください。 none_seekable で処理することのメリット   seekable ではなく none_seekable で処理することのメリットとして、 Web ブラウザでの処理を例に挙げて説明します。  もしもブラウザの処理が全て seekable であった場合、 ブラウジングスピードが遅くなることが予想されます。  なぜなら、Web ブラウザは、サーバから HTML をダウンロードし、 HTML 内のリンクを抽出し、そのリンクをさらにダウンロードします。 そしてリンクが画像の場合、画像をデコードして表示します。  画像のデコード処理が none_seekable であるならば、 画像データのダウンロード開始と同時にデコード処理が開始でき、 画像データのダウンロード終了とほぼ同時にデコード処理を完了できます。  一方でもしも画像のデコード処理が seekable だった場合、 画像データをダウンロード終了してからデコード処理を行なわなければならず、 その分タイムロスになります。 さらに欠点はタイムロスだけでなく、 画像データの全てをダウンロードして一旦 RAM やストレージに格納しておく必要があり、 その分のリソースを消費することになります。  画像データのサイズなんてイマドキのハードウェアスペックなら無視できる、 という意見もあるかもしれませんが、例えば 8K の低圧縮画像などは軽く数 10MB を越えます。 こういった画像のデータを全てダウンロードしてからデコードするなんてしてたら、 無駄にリソースを消費することが分かると思います。  また、最近はほとんど使われていませんが、 progressive JPEG なんて画像フォーマットが使われていた時期がありましたが、 これは none_seekable で処理して始めて意味のあるものです。  progressive JPEG を簡単に説明すると、 画像データの一部をダウンロードするだけで、低解像度の画像をデコードできる技術で、 ダウンロードが進むごとにデコード結果の解像度が上がるというものです。  これは、ネットワークの通信速度が低速なころに使用されていた画像フォーマットで、 いまではほとんど使われなくなったものですが、 none_seekable で処理しなければ全く意味のないものです。  他にも none_seekable で処理することのメリットとして、 動画配信に代表されるストリーミングサービスがあります。  あれも、 none_seekable が前提にあるからこそ可能なサービスです。  「ストリーミングサービスが none_seekable だ」と書くと 「Youtube はシークできるぞ」とかツッコミがあると思うので一応補足しておきます。  たしかに Youtube などの動画配信サービスはシークできるのが当たり前です。 しかし、通常再生時は none_seekable で処理していて、 シークなどの操作が入った時だけ、 サーバからデータをダウンロードしなおして処理しています。 つまり、基本は none_seekable です。  もしも動画データが seekable 前提だった場合、 動画データを全てダウンロードしてからでないと再生できないか、 seek 処理が大量に発生してサーバ間の通信負荷が非常に高くなることが予想されます。  また、seekable(randam access) は none_seekable(sequential) と比べて 非常にパフォーマンスが悪くなるのが一般的です。  例えば HDD の randam access は sequential と比べて 2 桁以上のパフォーマンス劣化、 SSD でも 1 桁以上劣化します。 RAM であっても、randam access することでキャッシュミスが発生しやすくなり、 パフォーマンス劣化からは逃れられません。 現代ではほとんど使われませんが、 テープデバイスなんて使った日には、どれほどかかるか想像すら出来ません。 データフォーマット   stream を処理する際に、 それを none_seekable として扱うには、 stream に流れるデータのフォーマットが none_seekable として 扱い易い構造になっている必要があります。  データフォーマットが none_seekable として扱い難い構造の場合、 上記のように「目標のパフォーマンスが出ない」、「必要なワークメモリが規定を越えてしまう」 という問題が発生する可能性があります。  ある程度の大きさになるデータフォーマットを定義する時は、 必ず none_seekable で処理することを考えて定義しましょう。  なお、 stream で処理することが多い画像や音声などのデータフォーマットは、 基本的には none_seekable で処理できるように定義されています。  もしもそうでなければ、放送や動画配信でデジタルデータを扱うことは出来ません。  ちなみに、データの encode と decode の none_seekable での扱い易さは、 相反することがあります。  その場合、どちらかを優先するか、折衷案の検討が必要です。 一つ言えることは、作業バッファを 0 にすることはまず不可能なので、 どの程度の作業バッファサイズなら妥当かを判断することが重要です。 例外   none_seekable で処理することで、 ダウンロードとデコードを同時に処理できるため高速に処理できる、と説明しましたが、 一部例外があります。  それは、専用ハードウェアを使用してデコードする場合です。  HDD レコーダなどの家電製品では、 動画や音声を処理する専用ハードウェアを搭載しています。 それら専用ハードウェアは、データを渡すと高速に処理して結果を返してくれますが、 処理するデータは全て揃えてから渡さなければならない、 という制約があることがほとんどです。  その場合は、none_seekable でダウンロードとデコードを同時に処理するよりも、 専用ハードウェアを使用して処理する方が高速に処理できます。  ただし、当然専用ハードウェアであるため、処理できるデータは限られていますし、 そのような専用ハードウェアが利用できる環境は限られています。 まとめ   stream を扱う際は、次を注意する必要があります。    極力 none_seekable で扱う    データフォーマットを決める時点で、 none_seekable で扱えることを考慮する   最後に   なんでこんなことを書いたかというと、 最近とある画像コーデックのライブラリを扱うことがあったんですが、 そのライブラリへの入力が seekable であることを前提としていてムカついた、 という経験をしたためです。  データ streaming 処理を行なう場合の基本的な考えなので、 必ずこれらを考慮に入れて設計するようにお願いします。  以上。 ","id":15,"section":"posts","summary":"これは seekable な stream と none_seekable な stream の使い分けに関する記事です。 使い分けが十分出来ている人は読まなくても大丈夫です。 皆さんは bitstream という単語をご存知でしょうか？","tags":null,"title":"stream は rewind/seek できる？","uri":"https://ifritjp.github.io/blog2/public/posts/2019/2019-07-10-stream/","year":"2019"},{"content":"  コレ を作るにあたって、データの serialize/deserialize の方法を調べた結果、 marshmallow_dataclass に落ち着きました。  いくつか調べた中で、パッと見、直感的に出来そうだった、というだけの理由ですが。。  実際、面倒な処理はほとんど無く、 serialize/deserialize が可能になりました。 使い型   marshmallow_dataclass は、 クラスを宣言する際に @dataclass デコレータを付けて宣言し、 メンバの型を宣言するのが基本です。  こんな感じ。 @dataclass class LogItem: # ゲームタイトル title:str # 日付 date:int # テキスト text:str # テキスト長 len:int   メンバの宣言が python っぽくないと思う方もいるかもしれませんが、 静的型付け言語になれていると、こっちの方が馴染み易い気がします。  JSON 化する場合は、 次のようにクラスメソッドに JSON 化するクラスのインスタンスを渡すだけです。 item = LogItem( \u0026#34;title\u0026#34;, time.time(), \u0026#34;text\u0026#34;, len( \u0026#34;text\u0026#34; ) ) print( marshmallow_dataclass.class_schema( LogItem )().dumps( item ) )   逆に JSON からクラスインスタンスを生成するには、 次のようにクラスメソッドに渡すだけです。 marshmallow_dataclass.class_schema( LogItem )().loads( text )   とても簡単です。  ただ、躓いた点があったので、気をつけるべき点として書いておきます。    python3.7 以降を使用する    @dataclass デコレータを付けたクラスに次を宣言してはならない    コンストラクタ init    @staticmethod load()     ","id":16,"section":"posts","summary":"コレ を作るにあたって、データの serialize/deserialize の方法を調べた結果、 marshmallow_dataclass に落ち着きました。 いくつか調べた中で、パッと見、直感的に出来そうだった、というだけの理","tags":null,"title":"python のクラスを JSON 化","uri":"https://ifritjp.github.io/blog2/public/posts/2019/2019-07-04-python-json/","year":"2019"},{"content":"  4 月頃から、英語のレベルを上げるため参考書を買って勉強をしている。  その参考書を使った最低限の基礎英語の復習は終ったので、次のステップに進むことにした。  基礎が終った後の学習方法には何が良いのか色々と調べてみたが、 色々な意見があるが最終的には「持続できるものが良い」というのが結論のようだ。  まぁ、「持続すること」が英語学習で最も難しいことは、 私自身が何度も挫折した経験があるので認識している。  そもそも、先日まで復習していた参考書もかなり眠い目をこすりながらやっていて、 このまま参考書を進めていっても、間違いなくまた挫折することは明らかだった。  じゃぁ、何が一番持続できるか？と考えた時、自分にはゲームが良いだろう。 という結論になった。 海外ゲームによる英語学習   ゲームのジャンルは、いわゆるノベルゲームあるいはアドベンチャーゲーム。 有名どころで STEINS;GATE と言えば通じるだろうか？ これなら文章量がハンパないので、勉強量という意味では問題ないだろう。 「ゲームでの英語学習は効率が悪い」という意見があるが、 「持続すること」が一番重要なので、「効率の悪さ」はこの際無視することにした。  ともかく今は、英語の文章を英語のまま解釈できるようになるため、 英語の文章をひたすら入力していくのが最も重要な期間で、 その期間を挫折せずにやりすごすためにも、「持続すること」が一番重要と考えている。  「習うより慣れよ」、脊髄反射できるまでの試練だ。  そもそも、本当に効率の良い英語学習方法が存在するのであれば 文科省もバカじゃないんだからその方法を採用しているはず。 そして、その学習方法に本当に効果があれば、 日本人の大多数が英語が出来ないまま放置されているはずがない。 しかし、現実問題として私を含め日本人の大多数が英語が出来ないままである。  つまり「日本国内における英語学習方法の違いによる効率の差」は、 ほとんど誤差レベルなんだと思う。  もちろん、「英語しか話せない人の中で生活すること」と、 「日本国内で独学で英語を学習する」のとでは、英語の習得効率に明らかな違いはあるだろう。 しかし、「日本国内で独学で英語を学習する方法」は、 どのような学習方法であっても、どれも大差ないレベルなんだと思う。 海外ゲームによる英語学習における問題   ということで、先週辺りから海外のゲームをプレイしているんだが、一つ問題がある。  その問題とは「reading の経験値しか得られない」ということだ。  日本のゲームの海外移植版をやるのがとっかかり易いと考えてたが、 そのゲームの TEXT は英語だが音声は日本語のままだった。 てっきり海外移植版なら音声も英語になっているものだと思っていたが、完全に想定外だった。  それならば、海外制作のオリジナルゲームなら音声も英語だろう、と思って探したが、 そもそも海外制作のノベルゲームやアドベンチャーゲームというジャンルはほとんど無かった。 あっても、音声がないという状況だ。 もちろん、他のジャンルのゲームなら英語音声のものはある。 しかしそのようなゲームは、TEXT の量的問題や、 そもそも英語とか関係なくゲームが進んでいって、 ほとんど単にゲームをプレイしているだけになってしまう、という問題がある。  ちなみに、海外ドラマや映画を学習手段として試したことがあるが、 アレはスピードが速すぎて、 自分のレベルではとてもではないけどハードルが高過ぎるという結論になっている。  自分のペースで進められる、というのが、 ノベルゲームやアドベンチャーゲームの良いところだ。  もちろん、内容が面白く持続できるということが前提だが。  なお、今回海外のゲームを探してみて初めて気が付いたことだが、 海外でアドベンチャーゲームというと、日本のアドベンチャーゲームとは全く違って、 アクションゲームがアドベンチャーゲームに分類されていた。  まぁ「adventure」 は「冒険」なんだから、当然といえば当然だろうが。  だいぶ前置きが長くなったが、 そんな訳で、多くの時間を費やして「reading の経験値しか得られない」のは勿体無いので、 「どうにかしてゲームに音声を付けよう」と思い、今回のツール制作に至った。 ゲームに音声を付ける手段   ここで想定するゲームは、 メッセージを表示する領域があり、クリックすることでメッセージが更新されて、 ストーリーが進んでいくものだ。  このメッセージを取り出し、機械音声でしゃべらせる。  もう少し技術的にいうと次になる。    スクリーンショットでゲーム画面をキャプチャ    キャプチャしたゲーム画面からメッセージ領域を判定し    メッセージ領域内のメッセージ画像を抽出し    抽出したメッセージ画像を OCR にかけて TEXT に変換し    変換した TEXT を Text To Speech で音声化する    上記を GUI でコントロール    メッセージ画像の抽出は OpenCV、 OCR は Tesseract OCR、 Text To Speech は Windows10 標準の SAPI.SpVoice を利用する。  クラウドサービスの API を使えば、これらを全て行なってくれるものもありそうだが、 今回は上記の技術を組み合わせで自前で作成する。  まぁ、自分で作ること自体も面白そうだし。  なお、お手軽に作るため、開発言語は Python とする。  プログラミング言語として、個人的にはあまり Python は好きではないんだけど、 手軽でさまざまなライブラリが揃っていて情報量も豊富、という意味では、 今は Python に敵う言語はないんじゃないかと思う。  なんだかんだ言っても、プログラミング言語はツールにすぎないので、 目的の物を簡単に作れるのが一番良い。 特に趣味で作るケースでは。  業務で使う場合は、 「チョット待て、他の言語はちゃんと検討したのか？」と言っておく。  自分で開発している LuneScript も、 lua VM 上で動作する大規模アプリを開発するには向いているけど、 使えるライブラリは皆無(Lua 用ライブラリは使えるけど、まともに使うには module 宣言が必要) なので、残念ながらこういう用途には向いていない。  ちなみに、 cygwin 版 python で作業しようと思ったが、 pip がどうにもこうにも期待通りに動作しなかったので、 普通の windows 版 python にした。  以降では、各技術について補足する。 スクリーンショット   スクリーンショット用に次をインストールする。 $ pip install pywin32 $ pip install Pillow $ pip install pyscreenshot   pywin32 は、 win32gui で特定の Window の領域を取得するために必要。  具体的には次のような感じ。 def getImageOf( window_title ): rect = win32gui.GetWindowRect( win32gui.FindWindow(None, window_title ) ) return ImageGrab.grab().crop( rect )  OpenCV   次の処理を OpenCV で行なう。    ゲーム画面からメッセージ領域を判定    メッセージ領域内のメッセージ画像を抽出    ちなみに OpenCV のインストールは次で出来る。 $ pip install opencv-python  OCR (Tesseract OCR)   次の処理を Tesseract OCR で行なう。    抽出したメッセージ画像を OCR にかけて TEXT に変換    Tesseract OCR は、次の URL からバイナリをダウンロードしてインストールし、  https://github.com/UB-Mannheim/tesseract/wiki  さらに python から利用するためのパッケージをインストールする。 $ pip3 install pyocr  Windows10 Text To Speech (SAPI.SpVoice)   次の処理を SAPI.SpVoice で行なう。    変換した TEXT を Text To Speech で音声化する    \u0026lt;https://github.com/mhammond/pywin32/releases\u0026gt; から、 python のバージョンに合う win32com モジュールのインストーラをダウンロードし、 インストールする。  SAPI.Speech の制御方法は、次の URL を参考に。  \u0026lt;https://www.daniweb.com/programming/software-development/code/217062/text-to-speech-using-com-python\u0026gt;  この SAPI.SpVoice の音声は、 一昔前の合成音声に比べればだいぶマシに聞こえるが、やはり違和感を感じる。  英語が出来ない自分が、英語の音声に違和感の文句を云うのもどうかと思うが、 やはりイマドキの最新の Text To Speech 技術と比べると、品質が落ちる。  そこで、Text To Speech の部分はクラウドサービスを使って違和感の緩和を検討する。 これについては後日取り上げる。 GUI   GUI は tkinter を利用する。  用途は次の通り。    ゲームの Window 指定    OCR のトリガ    OCR 後のメッセージ表示 \u0026amp; 編集    音声再生制御 (再生スピード,音量)   ログ   折角なので、学習の履歴を残す。  履歴は、日付、OCR 結果、全文字数 で、JSON 形式で残す。 欠点   このシステムの一番の欠点は、読み上げられる音声に全く感情が入らないってことだろう。 ゲームのト書部分なら無感情でも問題ないが、 セリフが無感情で読み上げられるのは、いささか味気ない。 まぁ、そこは割り切るしかないが。 今は、クリアに音声が聞こえる事の方が重要だろう。 感情がどうこういうのは、 実力が付いてから海外ドラマや映画を見るようにすれば良い話だ。 最後に   専門知識がなくても、フリーの技術を組合せるだけで、 これだけのものが作れるようになったというのはスゴい時代になったものだ。  ちなみにソースは \u0026lt;https://github.com/ifritJP/game-message-tts.git\u0026gt; にある。 興味があれば。 ","id":17,"section":"posts","summary":"4 月頃から、英語のレベルを上げるため参考書を買って勉強をしている。 その参考書を使った最低限の基礎英語の復習は終ったので、次のステップに進むこ","tags":null,"title":"ゲームのメッセージ欄に表示されたメッセージの読み上げシステム","uri":"https://ifritjp.github.io/blog2/public/posts/2019/2019-06-28-text-to-speech/","year":"2019"},{"content":"  だいぶ前に買って放置していた Raspberry pi zero w をセットアップしました。  Raspberry pi zero w と言えば「小型軽量」が売りなんで、 今回はポータブルな IOT デバイスとして使う事を目的として、 Bluetooth の機能(ファイル転送、 IP over Bluetooth) のセットアップをしました。  イマドキ Bluetooth なんて、 最新のイメージでセットアップすればすぐに使えるだろうと思って余裕でした。 しかし、実際には目的の機能が動作するまでに、かなりの時間が掛ってしまいました。  少なくとも、パッケージをインストールするだけでは済まず、 いくつかのファイルを編集 \u0026amp; コマンド実行が必要です。  そんな訳で、次に同じことをする時のために備忘録を残しておきます。  この記事で扱うメインは以下の通りです。    Raspberry pi zero w を USB 接続のみでセットアップ    Bluetooth によるファイル送受信機能(OBEX File Transfer)の実現    IP over Bluetooth (PAN) による、PC との SSH 接続確立    スムースにいけば、 作業時間は 10 〜 20分程度で完了します。 (OS イメージ書き込みや apt 更新などの待ち時間は除く)  なお、 Raspberry pi の設定を行なうホスト環境は Ubuntu 18.04.2 LTS とします。  Ubuntu が Native で動作する PC でも、 Windows 上の Gest OS の Ubuntu でも構いません。 ただし、 Windows 10 の subsystem の linux は対象外です。 Raspberry pi zero w を USB 接続のみでセットアップ  SD カードに OS Image を書き込む   公式サイトから OS Image を落して SD カードに書き込みます。  今回は Raspbian Stretch with desktop and recommended software の 2019-04-08 を使用しました。  以前 raspi で Bluetooth を扱った時、 Lite では意図する動作にならなかったトラウマがあるため、今回はこれを使用します。  イメージを書いたら、ssh と IP over USB (RNDIS) を有効化するため、 SD カードをマウントした直下の次のファイルを編集します。    ssh    config.txt    cmdline.txt    編集内容については、次の URL を参考に。  \u0026lt;https://qiita.com/mt08/items/ce5e3911d74d7fad4563#%E6%89%8B%E9%A0%86\u0026gt;  念の為要点だけをまとめておくと、    空の ssh ファイルを作成    config.txt に次を追加   dtoverlay=dwc2     cmdline.txt    rootwaitとquietの間に次を挿入     modules-load=dwc2,g_ether  RNDIS 設定   Ubuntu では、Raspberry pi zero w (以降 raspi) を USB (2つある USB コネクタのうち、 HDMI コネクタ側の方)で接続すれば、 運が良ければ特になにもせずに IP over USB (RNDIS) で raspi と通信可能になります。  通信可能かどうかは、次の方法で確認できます。 $ ip a   ここで enp0s20u1 的なデバイスが表示されていて、 IP アドレスが取れていることを確認します。  IP アドレスが取れている場合、次のコマンドで raspi の IP を確認します。 $ ip n   同じサブネットのアドレスがあれば、それが raspi の IP。  raspi の IP が分かったら、 ssh すれば OK。 $ ssh -Y pi@10.42.0.100   ちなみにデフォルトパスワードは raspberry.  大抵の場合、運が良くないので上記の確認では期待した結果にならない。  そのため、次のネットワーク設定が必要になる。  まず、ネットワーク設定を行なう前に、現在のネットワークの状況を確認します。 $ ip a   このコマンドで表示される「デバイス名」と「MAC アドレス」をメモっておきます。  メモった後に、次のコマンドを実行します。 $ sudo nmtui   起動すると、いくつかの Ethernet 設定がリストで表示されるので、 編集を選択します。  編集を選択すると、デバイスの欄に「デバイス名」あるいは「MAC アドレス」が 表示されているので、 USB の方の情報が表示されている Ethernet 設定を見つけます。  設定を見つけたら、一旦その設定自体を消します。 USB のデバイスに関する設定が複数ある場合は、全て削除します。  そして、新しく設定を追加します。  このときの設定内容は次の通りです。    接続タイプ Ethernet    デバイス名を enp0s20u1 (実際のデバイス名に合せる)    IP4 config を share にする    Require IPv4 addression for this connection をチェック    設定後、connection を activate する。  これで再度 ip a から確認してください。 これでも上手く動作しない場合、 deactivate と activate を何度か繰り返すと解消されることがあります。  ちなみに Windows をホストに作業する場合、野良ドライバのインストールが必要です。 個人的には、Windows への野良ドライバインストールはオススメできません。  以降は、 raspi に ssh 接続した状態で作業します。  まずは、次のコマンドで apt を更新しておきます。 $ sudo apt-get update $ sudo apt-get install bluez-tools pulseaudio-module-bluetooth   本来 pulseaudio-module-bluetooth は、 audio sink 用のものなので、 今回の目的には不要のはずなんですが、 これがないとペアリング後の接続すら出来なかったので入れておきます。  次に、 raspi のホスト名を変更します。 このホスト名が、 bluetooth のペアリングのときに使用されます。  次のコマンドを実行し、 Network Options -\u0026gt; Hostname で適当に変更します。 $ sudo raspi-config  Bluetooth によるファイル送受信機能(OBEX File Transfer)の実現   Bluetooth のファイル送受信には、 追加で obex 系の設定が必要となります。 $ sudo apt install obexpushd   obex 系の処理を動かすには、 bluetoothd に –compat オプションを必要です。  オプションの指定は次のように /etc/init.d/bluetooth に –compat を追加します。 #SSD_OPTIONS=\u0026#34;--oknodo --quiet --exec $DAEMON -- $NOPLUGIN_OPTION\u0026#34; SSD_OPTIONS=\u0026#34;--oknodo --quiet --exec $DAEMON -- --compat $NOPLUGIN_OPTION\u0026#34;   あるいは、 /etc/systemd/system/bluetooth.target.wants/bluetooth.service に追加するケースもあります。 #ExecStart=/usr/lib/bluetooth/bluetoothd ExecStart=/usr/lib/bluetooth/bluetoothd --compat   ファイル編集後 –compat オプションを反映させます。 $ sudo systemctl daemon-reload $ sudo /etc/init.d/bluetooth restart $ sudo systemctl restart bluetooth   次に Bluetooth ファイル受信用ディレクトリを作成します。 $ mkdir ~/bluetooth   そして次のコマンドを実行します。 $ sudo /usr/bin/obexpushd -B -n -o /home/pi/bluetooth   これでホスト PC からファイルを送信すると、 /home/pi/bluetooth にファイルを受信します。  なお、obexpushd は次のようにサービスとして登録します。  /etc/systemd/system/bt-obexpushd.service に次の内容をもつファイルを作成。 [Unit] Description=Bluetooth obexpushd After = bluetooth.service [Service] ExecStartPre=/bin/sleep 4 ExecStart=/usr/bin/obexpushd -B -n -o /home/pi/bluetooth Type=simple [Install] WantedBy=multi-user.target   サービスを有効化 $ sudo systemctl enable bt-obexpushd $ sudo systemctl start bt-obexpushd  IP over Bluetooth (PAN) による、PC との SSH 接続確立   PAN の設定は、次の URL の回答をそのまま設定すれば OK です。  \u0026lt;https://raspberrypi.stackexchange.com/questions/29504/how-can-i-set-up-a-bluetooth-pan-connection-with-a-raspberry-pi-and-an-ipod\u0026gt;  なお、上記 URL の内容を設定後、再度ペアリングをやり直してください。 ","id":18,"section":"posts","summary":"だいぶ前に買って放置していた Raspberry pi zero w をセットアップしました。 Raspberry pi zero w と言えば「小型軽量」が売りなんで、 今回はポータブルな IOT デバイスとして使う","tags":null,"title":"Raspberry pi zero w で Bluetooth 色々(ファイル転送:obex、 IP over BT:PAN )","uri":"https://ifritjp.github.io/blog2/public/posts/2019/2019-06-15-rasp0w/","year":"2019"},{"content":"  VMWare のディスクイメージのサイズは、Gest OS 上のディスクサイズと異なる。  基本的には、Gest OS 上で実際に使用されているサイズに圧縮された形でホスト OS 上に保持される。  しかし、 Gest OS 上でファイル作成、削除を繰り返していると、 Gest OS 上での使用サイズよりも、 ホスト OS 上でのディスクイメージサイズがかなり大きくなっていることがある。  このような状態になった時に、ホスト OS 上のディスクイメージサイズを、 Gest OS 上での使用サイズ程度に削減するツール(vmware-toolbox-cmd)が vmware から提供されている。  通常は、この vmware-toolbox-cmd を使うことで圧縮されるはずなのだが、 自分の環境では全くサイズが変わらなかった。  いくつか試した結果、削減出来た方法をメモしておく。 Gest OS 上でのディスクのクローン   今回実施した方法は Gest OS 上でのディスクのクローンを作成することだ。  ある意味分かりきった方法かもしれない。  ただ、クローン作成の方法はファイル単位のコピーではなく、 dd コマンドによるクローン作成 で上手くいった、 ということは意外と言えるんじゃないだろうか？  ファイル単位のコピーだと、コピーにかなり時間がかかると思うが、 dd コマンドで済んだので、10 GB 近いコピーも比較的短時間でコピーが出来た。  dd コマンドは、特に何か特別なオプションを付けて実行したのではなく、 普通に実行しただけだ。  念の為、作業手順をまとめておく。 作業手順     クローン先の空のディスクイメージを作成する    ディスクイメージを VMWare に登録する    Gest OS を起動する    vmware-toolbox-cmd を使って圧縮   vmware-toolbox-cmd disk shrinkonly     Gest OS 上での圧縮対象ディスクと、クローン先のデバイス名をメモる    dd コマンドでクローン作成   dd if=/dev/圧縮対象 of=/dev/クローン先 bs=1M     ここで指定するドライブは、パーティションではなくドライブ全体を指定すること。      Gest OS を shutdown    ここでクローン先のディスクイメージを見て、 Gest OS 上の使用量とほぼ同じサイズに削減されていることを確認する。 もしも削減されていない場合、これ以降の作業には意味はない。      圧縮対象ディスクイメージを VMWare から除外し、 代わりにクローンしたイメージを登録する。    この時クローンイメージを割り付けるハードウェアの ID などが、 元の圧縮対象ディスクイメージと同じになるように登録する。      Gest OS を起動する。    以上の手順により、サイズが圧縮されたクローンのイメージで運用できる。 ","id":19,"section":"posts","summary":"VMWare のディスクイメージのサイズは、Gest OS 上のディスクサイズと異なる。 基本的には、Gest OS 上で実際に使用されているサイズに圧縮された形でホ","tags":null,"title":"VMWare ディスクイメージが圧縮されないときの対応方法","uri":"https://ifritjp.github.io/blog2/public/posts/2019/2019-05-20-vmware/","year":"2019"},{"content":"  現在 LuneScript の C 言語へのトランスコンパイル処理を対応中だが、 トランスコンパイルする際に関数ポインタによる関数コールのオーバーヘッドが どの程度なのか気になったので調べてみた。 結果   初めに結果から書くと、 関数ポインタによる関数コールのオーバーヘッドは、 通常の関数コールに比べて約 1.267 倍となることが判った。   この数値は、あくまで今回の実験結果であって、 関数ポインタかどうかの違いだけはなく、他の要因も入ってしまっている。 また、実行環境によっても差は出てくるだろう。  しかし、それでも目安程度にはなるだろう。 所感   論理的に考えて、関数ポインタの関数コールが通常の関数コールに比べて 遅くなることは理解していたが、これまで調べたことはなかった。 それが、今回の実験で明かになった。  個人的にはもっと差が出るかと思ったが、案外少ない結果になった。 これは、実験用コードが小さ過ぎて全てキャッシュに乗ってしまっているのが一番の要因だとは思う。 とはいえ、明らかなオーバーヘッドがあることには違いない。  プログラミングをしていれば感じていることだと思うが、 プログラムは関数コールの塊だ。  つまり、関数コールのオーバーヘッドは、 そのままプログラム全体の性能低下に直結する。  「関数ポインタ」というと、あまり使わっていないイメージを持つ人も多いかもしれないが、 オブジェクト指向の「ポリモーフィズム」あるいは「多態性」というと、 良く使っているイメージがあるのではないだろうか？  関数ポインタなど動的に動作が変わる処理は、 目的の制御を実現する上で非常に重要だが、 コードの把握が難しくなったり、オーバーヘッドによる性能低下を引き起こす可能性がある。  関数ポインタと通常の関数は、その特性にあわせてどちらを使用するかの検討が必要だ。  今回の実験結果をうけて、それがより明らかになったと思う。 実験詳細   ここでは、今回の実験方法について説明する。 コード   実験用に次の C 言語コードを作成した。 void sub( void ) { } void func_direct( func_t * pFunc ) { sub(); } void func_indirect( func_t * pFunc ) { pFunc(); }   func_direct() は sub() 関数を直接コールする関数で、 func_indirect() は sub() 関数を関数ポインタでコールする関数だ。  この両者の関数を実行したときの実行時間を比較している。  ちなみにコードの全体は次の通りである。 #include \u0026lt;sys/time.h\u0026gt;#include \u0026lt;time.h\u0026gt;#include \u0026lt;stdio.h\u0026gt; typedef void (func_t)( void ); double getTime( void ) { struct timeval tm; gettimeofday( \u0026amp;tm, NULL ); return tm.tv_sec + tm.tv_usec / 1000000.0; } void sub( void ) { } void func_direct( func_t * pFunc ) { sub(); } void func_indirect( func_t * pFunc ) { pFunc(); } void func_none( func_t * pFunc ) { } int main( int argc, const char * argv[] ) { long long loop; const char * pMode; double prev = getTime(); switch ( argc ) { case 1: pMode = \u0026#34;indirect\u0026#34;; for ( loop = 0; loop \u0026lt; 1000 * 1000 * 1000 * 2; loop++ ) { func_indirect( sub ); } break; case 2: pMode = \u0026#34;direct\u0026#34;; for ( loop = 0; loop \u0026lt; 1000 * 1000 * 1000 * 2; loop++ ) { func_direct( sub ); } break; case 3: pMode = \u0026#34;none\u0026#34;; for ( loop = 0; loop \u0026lt; 1000 * 1000 * 1000 * 2; loop++ ) { func_none( sub ); } break; } printf( \u0026#34;%s: time = %g\\n\u0026#34;, pMode, getTime() - prev ); return 0; }   このプログラムは、コマンドラインの引数によって func_direct(), func_indirect(), func_none() のいずれかを 所定の回数分実行し、実行時間を表示する。  ちなみに func_none() は、関数ポインタと通常の関数コールの差を出す際に、 できるだけ他の要因を除外するために作成した関数だ。 計測結果  indirect: time = 11.4617 indirect: time = 11.2905 indirect: time = 11.2595 indirect: time = 11.3391 indirect: time = 11.3123 direct: time = 10.5253 direct: time = 10.5927 direct: time = 10.5389 direct: time = 10.6043 direct: time = 10.5259 none: time = 7.64467 none: time = 7.60627 none: time = 7.75474 none: time = 7.60123 none: time = 7.63887   これは、コマンドライン引数を変えて上記のプログラムをそれぞれ 5 回ずつ実行した結果だ。  それぞれを平均すると次のようになる。     時間(秒) 関数コールの時間(秒)     関数ポインタ 11.333 3.683   通常関数コール 10.557 2.908   関数コールなし 7.649      上記の「関数コールの時間」は、計測した時間から「関数コールなし」の時間を引いたものだ。  つまり、 for 分の制御などの関数ポインタのオーバーヘッドとは直接関係ない処理の時間を引いている。  この結果をもとに、次の計算をすると  (/ 3.683 2.908) 1.266506189821183  関数ポインタによる関数コールのオーバーヘッドは、 通常の関数コールに比べて 約 1.267 倍 となる。  以上 ","id":20,"section":"posts","summary":"現在 LuneScript の C 言語へのトランスコンパイル処理を対応中だが、 トランスコンパイルする際に関数ポインタによる関数コールのオーバーヘッドが どの程度なのか","tags":null,"title":"関数ポインタのオーバーヘッド","uri":"https://ifritjp.github.io/blog2/public/posts/2019/2019-05-19-func-pointer/","year":"2019"},{"content":"  とある事情で使い続けていた emacs23.4 (2012/1) を、 先日 emacs26.2 (2019/4) にアップデートした。  このとき gdb 周りの設定を変更する必要があったので、備忘録としてまとめておく。  *2019-06-12: my-gud-stop, my-gud-mode-func を追加*  *2019-08-26: dedicate の抑制追加*  なお、M-x gud-gdb で起動すれば従来形式のインタフェースが利用できるが、 ブレークポイントが表示されない等の不具合があるので M-x gdb を利用する。  それにしても、新しい M-x gdb のインタフェースは emacs っぽくないと思うんだけど、 オレがおっさんだからそう思うんだろうか？ emacs の gdb 設定  ;; gud-overlay-arrow-position が nil だとエラーするので。。 (setq gud-tooltip-display \u0026#39;((and gud-overlay-arrow-position (eq (tooltip-event-buffer gud-tooltip-event) (marker-buffer gud-overlay-arrow-position))))) ;; gdb バッファの C-c C-c で、プログラムを停止させる。 (setq gdb-gud-control-all-threads nil) ;; input/output バッファが勝手に表示されるのはウザいので、抑制 (setq gdb-display-io-nopopup t) ;; input/output バッファが dedicate されるのはウザいので、抑制 (defadvice gdb-display-buffer (around gdb-display-buffer) (let (window) (setq window ad-do-it) (set-window-dedicated-p window nil) window )) (ad-activate \u0026#39;gdb-display-buffer) ;; gdb バッファの C-c C-c ではプログラムが停止しなかったので、修正 (defun my-gud-stop () (interactive) (comint-interrupt-subjob) (gud-stop-subjob) ) ;; 上記 my-gud-stop 関数を C-cC-c に登録する関数 (defun my-gud-mode-func () (define-key (current-local-map) \u0026#34;\\C-c\\C-c\u0026#34; \u0026#39;my-gud-stop) ) ;; フックに登録 (add-hook \u0026#39;gud-mode-hook \u0026#39;my-gud-mode-func)   以降で、上記の設定について説明する。 gud-tooltip-display   1 つ目は、単純に gud.el の不具合のような気がするが、 tooltip を表示する処理を修正している。  gud-tooltip-display は、 gud で tooltip を表示する処理のようだが、 この処理で (make-buffer gud-overlay-arrow-position) を実行している。  この処理は、 gud-overlay-arrow-position が nil の時にも実行されるケースがあるようで、 その時にエラーにならないように and を追加している。 gdb-gud-control-all-threads   gdb-gud-control-all-threads は、 gud の制御を全スレッドに対して反映させるかどうかのフラグで、 emacs 23 ではデフォルト nil だった。  新しい gdb では、 gdb-gud-control-all-threads がデフォルト t になっている。  gdb-gud-control-all-threads が t だと、 どうにもこうにも意図したデバッグ制御にならなかったので nil とした。  なお、 C-c C-c でデバッグ対象プログラムを停止できるが、正常に動作しない場合がある。  その場合 M-x gud-stop-subjob してから C-c C-c すると、停止する。 gdb-display-io-nopopup   emacs23.4 の gdb は、 デバッグ対象プログラムの stdin/out と gdb の制御コマンドを、 一つのバッファで管理していた。  しかし、 新しい gdb は stdin/out と、gdb の制御コマンドを別々のバッファで管理している。  gdb-display-io-nopopup は、 stdin/out に変化があった際のポップアップ制御を抑制するかどうかのフラグ。  デフォルトだと t だが、 これだとソース編集中やステップ実行中に、 stdin/out のバッファが突然表示されてウザいので nil とした。  なお、gdb-display-io-nopopup を t とすると、 M-x gdb 実行時にも stdin/out のバッファが表示されないため、 stdin/out にアクセスする場合は 自分で C-x b 等で切り替える必要がある。  ちなみに stdin/out バッファの名前は *input/output of ...* 。 ここで … には、デバッグ対象のファイル名が入る。 my-gud-stop   emacs23.4 だと C-cC-c でプログラムを停止して (gdb) プロンプトが表示されたのだが、 emacs26.2 だと C-cC-c でプログラムを停止できない。  そこで、プログラムを停止する関数を作成している。 my-gud-mode-func   上記関数を C-cC-c に登録するための関数。  gud-mode 時にキーバインドを登録するように gud-mode-hook に追加。 dedicate   普通に使うと、 gud の input/output バッファの window が dedicate される。  dedicate されると、 C-x b などでバッファを切り替えられなくなる。  個人的にこれは使い勝手が悪いので、 dedicate されないように gdb-display-buffer の処理をかえる。  以上。 ","id":21,"section":"posts","summary":"とある事情で使い続けていた emacs23.4 (2012/1) を、 先日 emacs26.2 (2019/4) にアップデートした。 このとき gdb 周りの設定を変更する必要があったので、備忘録としてまとめておく。 *2019-06-12: my-gud-stop,","tags":["emacs"],"title":"emacs 更新に伴なう gdb の設定","uri":"https://ifritjp.github.io/blog2/public/posts/2019/2019-05-13-emacs26-gdb/","year":"2019"},{"content":"  たまたま見つけたブログの記事で気になったものがあったので、 自分の意見を書いておきます。 気になったブログの記事   「エンジニア就職志望者が情報工学科に行くのは間違いです！学べることが違います！」  \u0026lt;https://www.torikun.com/entry/engineer-jouhoukougaku\u0026gt;  この記事を要約すると、    大学の情報工学科のプログラミング単位取得だけでは 学習時間が足りないのでプログラミングスキルを上げるのは難しい。    スキルを上げるにはプログラミングスクールがオススメ    というモノです。  まぁ確かに、 大学の講義・実習だけで十分なプログラミングスキルを身に付けるのは不可能であるのは事実です。  とはいえ、『エンジニア就職志望者が情報工学科に行くのは間違い』というのは、 流石に異論があります。  ブログの著者と自分とで異なる意見になる理由を考えると、  【「エンジニア」という言葉の定義が違う】  から、だと思います。 「エンジニア」とは   上記の記事では、エンジニアには次の能力が必要だとしています。    プログラミングスキル    コミュニケーション能力    マネジメント能力    これらは確かに重要です。  というか、「コミュニケーション能力」や「マネジメント能力」は、 エンジニアでなくても社会で働くには必要な能力です。  つまりこの著者は、  【「エンジニア」に特化して必要な能力は「プログラミングスキル」だけ】  と主張しているように読めます。  これは、私の考えと完全に異なります。  まず、私が考える「エンジニア」像を説明します。  エンジニアとは、曖昧なゴールイメージを技術によってスマートな形で実現できる能力を持つ人。  例えば「家を建てる」というゴールイメージがあるとします。  家を建てることは情報系の「エンジニア」の仕事ではないと思いますが、 あくまで例として考えてください。  この場合、次のような様々なことを決定し、設計書を作成して建築する必要があります。    建てる場所    予算    広さ    デザイン    機能性    耐久性    拡張性    メンテナンス性    建材    日程    etc…    このように、 曖昧なゴールイメージを実現するために具体的な作業項目に分解し、 分解された作業の課題を洗い出し、 課題を解決し、 イメージを具現化する技術を持つのが、私が考える「エンジニア」です。  もちろん、現実には一人のエンジニアが全てを担当できる訳ではありません。  しかし、ブログの著者のような「エンジニア ＝ プログラミングスキルのある人 」では、 絶対にありません。  またブログの記事には、次の記載があります。 例えば、大学２年生の時にはフーリエ変換という数学の公式を習います。 この技術はパソコンの仕組みを突き詰めて行くと重要になってくる有名な数式です。 微分とか積分とかいろいろ難しい公式を覚えて問題を解いていきます。 エンジニアの方ならおわかりかと思いますが、 エンジニアとして仕事をする上でこのフーリエ変換を使う人はぜんぜんいません。   確かに全てのエンジニアが微分・積分を必要とする訳ではないです。 しかし、技術の背景を知っているエンジニアと、 プログラミングしか出来ないプログラマーでは、担当できる範囲が全く違ってきます。  たとえばディープラーニングなどの技術は、 プログラミングしか出来ないプログラマーでは 絶対 に作り出すことは出来ません。 様々な知識を持つエンジニアが集結してこそ可能なものです。  もちろん大学の講義レベルの知識だけで、すぐに何かが実現出来るということはありません。 しかし、大学の講義はさまざまな技術の基礎そのものであり、 その基礎を身に付けているかどうかで、その後の応用が出来るかどうかの違いに繋がってきます。  特に基礎部分は、体系的に学んだ方がより深い理解につながります。 そして大学の情報工学部の単位は、体系的に学ぶことが出来る構成になっています。  つまり大学の情報工学部は、『「エンジニア」になるためのもっとも早道である』と言えます。 認識が異なる理由   では、ブログの著者は何故「エンジニア ＝ プログラミングスキルのある人」という 認識なのでしょうか？  あくまで私の想像ですが、これは日本のソフトウェア開発業界の特色によるものだと思います。  その特色とは、いわゆる「ゼネコン方式」です。  大手が仕様を決め、実装を外部にアウトソーシングする。  ブログ著者にとって「エンジニア」とはアウトソーシング先であり、 「エンジニアは安い金額で実装さえ出来れば良い」という思考なのではないでしょうか？  日本には、このような思考が蔓延しているため、 エンジニアの待遇は良くならないし、 技術レベルも世界から離される一方なのではないでしょうか？  なお、ブログ著者のプロフィールを見ると、 IBM Tokyo Lab に務めているとあります。 いわゆる大手であるのは間違いないでしょう。 エンジニア就職志望者はどうあるべきか   私の考えは、「エンジニア就職志望者は様々な技術を学ぶべき」です。  「他人が作った仕様を元に、プログラムだけ組んでいれば幸せ」という人は、 ブログ著者が主張するようにプログラミングスクールなりに行けば良いと思います。  ただ、日本のゼネコン方式ソフトウェア開発を請け負う、 いわゆる SIer の給与は発注元の企業よりもかなり低いのが一般的です。 それこそ IBM の半分かそれ以下ではないでしょうか？ そのことは認識しておく必要があります。  なお、エンジニア志望者が行くべきなのは、情報工学科でなくても良いと思います。  というのも、私の「エンジニア」の定義は広いので、 情報工学科では収まりきらないためです。 何を極めたいかによって、何を学ぶべきかは変ってくるでしょう。  一つだけ必須技術を上げるならば、それは 「英語」 です。  今後の「エンジニア」業界で、 日本が世界をリードすることは極一部を除いて無いでしょう。  つまり、新しい技術は海外から導入することになります。 その時、その技術の解説は英語であるのが一般的です。  英語が出来れば、いち早く技術の導入が可能になります。  まぁ、これは今に始まったことではなく、 それこそコンピュータサイエンスという言葉が一般化したころから英語が標準でした。  ただ平成の時代は、    今よりは技術の進歩が激しくなく、日本語の翻訳を待っていてもまだどうにかなっていた    国内で働いているだけなら、外国人を相手にする機会がほとんどなかった    などの理由から「英語は出来た方が良い」というレベルでした。  しかし現在は、    技術の進歩が激しく、日本語の翻訳を待っていたら周回遅れどころか浦島太郎になる    ある程度新しい技術を取り入れる場合、国内の日本人だけで開発するのが難しくなった    などで、まともな「エンジニア」として働くには、英語はなくてはならない状況です。  もしもあなたがエンジニアを志す学生で、英語を苦手としているのならば、 留年してでも英語は習得しておくべきです。  世界と戦う意思のあるまともな日本の企業でエンジニアとして働くのであれば、 入社資格として英語のレベルを問われるでしょう。  逆に英語のレベルを不問とするような会社は、 世界と戦うことを諦めているか、 あなたを安く使える労働力と捉えているかのどちらかの可能性が高いです。  また、英語がまともに出来れば外資系や海外で働くことも選択肢になります。  英語習得のために大学を 1 年留年したとしても、 その後のエンジニア人生を考えれば充分おつりがくるでしょう。  英語が出来ない私だからこそ、 英語が出来ない現状がどれほどマズいことかを、 この歳になって身をもって感じています。  私はこれまで何度も英語の学習に挑戦と挫折を繰り返してきましたが、 今の状況なって本当にマズいことを実感し、 ラストチャンスとして人生で何度目かのトライをしています。  皆さんは、私のような思いをしないで済むように、英語だけは身につけてください。  もしかしたら、英語よりも中国語の方が良いかもしれませんが、 それはまだ何ともいえない状況です。 ","id":22,"section":"posts","summary":"たまたま見つけたブログの記事で気になったものがあったので、 自分の意見を書いておきます。 気になったブログの記事 「エンジニア就職志望者が情報工学","tags":null,"title":"『エンジニア就職志望者が情報工学科に行くのは間違い』は間違い","uri":"https://ifritjp.github.io/blog2/public/posts/2019/2019-04-18-engineer/","year":"2019"},{"content":"  外出先の暇な時間を有効利用するため、ドキュメント書きをしたくなることがあります。  そして私は emacs ユーザ。  emacs ユーザが書きモノをするといえば、 emacs/org-mode です。  ここでは、 Android で emacs/org-mode を使って qiita に投稿するまでの環境作りを紹介します。  ノート PC を持っている人は、普通にノート PC を持っていけば良いと思います。 用意するもの     タブレット    Bluetooth キーボード    タブレット用スタンド   環境構築  Android アプリ   まずは Android に次のアプリを入れます。    termux    ハードウェアキーボード配列変更アプリ (英語 or 日本語)    Hacker\u0026#39;s Keyboard    全て Root なしに Google Play で入れられます。  配列変更アプリは US 配列と JIS 配列でアプリが分かれているので、 好きな方を入れてください。 入れた後に、 Android の設定でハードウェアキーレイアウトを 「Ctrl、Caps 交換」に切り替えます。 なお、 Caps/Ctrl の入れ替えが不要な場合は、配列変更アプリを入れなくて良いです。  Hacker\u0026#39;s Keyboard は必須ではないですが、 他の IME では、ハードウェアキーボードと想定外の干渉をすることがあります。 termux 設定     ピンチイン、アウトでフォントサイズを変更    次のパッケージを termux にインストール    emacs    curl    git     emacs 設定  ~/.emacs/init.el 設定   次の内容の ~/.emacs/init.el を作成 (package-initialize) (add-to-list \u0026#39;package-archives \u0026#39;(\u0026#34;melpa\u0026#34; . \u0026#34;https://melpa.org/packages/\u0026#34;))  パッケージインストール   M-x package-list-packages で、次のパッケージを emacs にインストール    ox-qm    ddskk    helm    session    helm/session は必須じゃないけど、入れておいて損はない。 org-qiita.el インストール  $ git clone https://github.com/ifritJP/org-qiita-el   設定等の話は次を参考に。  \u0026lt;https://qiita.com/dwarfJP/items/594a8d4b0ac6d248d1e4\u0026gt; パッケージ設定  (show-paren-mode) (add-to-list \u0026#39;load-path (expand-file-name \u0026#34;~/work/org-qiita-el\u0026#34;)) (require \u0026#39;ox-qmd) (require \u0026#39;org-qiita) (setq org-qiita-token \u0026#34;XXXXXXXXXXXXXXXXXXXXXXXXX\u0026#34;) \u0026lt;---- qiita のトークン (org-qiita.el の説明参考) (setq org-qiita-export-kill-close t) (setq my-key-map (make-keymap)) (define-key global-map (kbd \u0026#34;C-z\u0026#34;) my-key-map) (define-key my-key-map (kbd \u0026#34;SPC\u0026#34;) \u0026#39;set-mark-command) (define-key my-key-map (kbd \u0026#34;i\u0026#34;) \u0026#39;helm-imenu) (define-key global-map (kbd \u0026#34;C-x b\u0026#34;) \u0026#39;helm-mini) (require \u0026#39;helm) (require \u0026#39;session) (require \u0026#39;recentf)   Android は Ctrl-SPC が、「キーボードレイアウト切替」になっています。  このため、 Ctrl-SPC がシステムに奪われて set-mark-command が動作しません。  暫定対応として、 C-z SPC に set-mark-command を割り当てました。 使い勝手はイマイチですが、意識してやればなんとか使えるレベルです。 最後に   簡単なドキュメント書きなら、これで十分です。  ノート PC と比べても、遜色ないレベルです。  とはいえ欠点もあります。    タブレットでの Web 検索がやり難い    タブレットは org-mode 専用で、検索は別途スマホでやる方が良いと思います。    ただ、検索結果をコピペするような場合は、タブレットでやった方が良いです。      C-SPC が使えない。    これは android の制約で、しかたがない？      次回は、外出先でのソフト開発に耐えられる環境について書きたいと思います。 ","id":23,"section":"posts","summary":"外出先の暇な時間を有効利用するため、ドキュメント書きをしたくなることがあります。 そして私は emacs ユーザ。 emacs ユーザが書きモノをするといえば、 emacs/org-mode です","tags":null,"title":"Android で emacs/org-mode/qiita 投稿","uri":"https://ifritjp.github.io/blog2/public/posts/2019/2019-03-23-termux-org/","year":"2019"},{"content":" forkwell の github 分析結果が面白かったので貼っておく。  ","id":24,"section":"posts","summary":"forkwell の github 分析結果が面白かったので貼っておく。","tags":["lua"],"title":"この度 Lua 神を拝命しました","uri":"https://ifritjp.github.io/blog2/public/posts/2019/2019-03-16-lua/","year":"2019"},{"content":"  Outlook は当初から評判が良くないため個人的には使用していません。 もうず〜〜〜〜〜と、 PC のメール環境は Mew を使用しています。  しかし、自分のメール送信・受信環境は好きなものを選べますが、 相手のメール送信・受信環境は選べません。  そしてつい先日も、 Outlook から送信されたメールで文字化けメールを受信しました。  どうして化けたのか気になったので、調べてみました。    メールの MIME に示されているコードは \u0026#34;gb2312\u0026#34; となっている    同じメールを Outlook で受信している人に聞いてみると、文字化けしていないと言う    emacs のコード変換で化けたのか？と思い、 メールを保存してブラウザの表示言語を簡体字中国語設定で表示してみると文字化けしなかった。    この時のブラウザのテキストエンコーディングを見てみると \u0026#34;GBK\u0026#34; だった      emacs で利用可能な文字コードを見てみると \u0026#34;gb2312\u0026#34; と \u0026#34;GBK\u0026#34; は別ものとして存在している。    試しに文字化けしたメールを、 emacs の \u0026#34;GBK\u0026#34; を指定して開くと文字化けしなかった    Wikipedia を見ると \u0026#34;GBK\u0026#34; は \u0026#34;gb2312\u0026#34; を拡張したものということが分った    また、 Microsoft が GBK を Windows コードページ 936 として定義した、との記載がある。      MS も Outlook で送信すると文字コード判定が間違えることを認識している    次の URL に記載されている「方法3」が、まさにそれの対処方法    \u0026lt;https://support.microsoft.com/ja-jp/help/881816\u0026gt;      以上のことから、次の事が考えられます。    Outlook で所定の文字を含むメールを送信する際、 Outlook の自動文字コード判定によって WCP936 として認識される。    WCP936 は本来 GBK であるが、メールの MIME には charset=\u0026#34;gb2312\u0026#34; として宣言される    メールを受信した Mew は、 MIME の情報を見て gb2312 として処理するが、 実際のメールは gb2312 ではなく GBK でエンコーディングされているため、文字化けする。   Mew での対応   Outlook のダメさ加減を嘆いてもしようがないので、 ここでは Mew で受信した時に化けずに表示できる対応をします。  対応コードは以下です。 (defun my-mew-change-gb2312-for-outlook () \u0026#34;outlook 対応。 Outlook の gb2312 は gbk になっている。。。\u0026#34; (setq mew-cs-database-for-decoding (mapcar (lambda (X) (if (equal (car X) \u0026#34;gb2312\u0026#34;) (list (car X) \u0026#39;gbk) X)) mew-cs-database-for-decoding))) (eval-after-load \u0026#34;mew\u0026#34; \u0026#39;(my-mew-change-gb2312-for-outlook))   以下で上記処理の説明をします。  Mew は MIME の charset と、 emacs の coding-system の紐付けを mew-cs-database-for-decoding で管理しています。  こんな感じ。 (defvar mew-cs-database-for-decoding `((\u0026#34;us-ascii\u0026#34; nil) (\u0026#34;iso-8859-1\u0026#34; iso-8859-1) (\u0026#34;iso-8859-2\u0026#34; iso-8859-2) (\u0026#34;iso-8859-3\u0026#34; iso-8859-3) (\u0026#34;iso-8859-4\u0026#34; iso-8859-4) (\u0026#34;iso-8859-5\u0026#34; iso-8859-5) (\u0026#34;iso-8859-6\u0026#34; iso-8859-6) (\u0026#34;iso-8859-7\u0026#34; iso-8859-7) (\u0026#34;iso-8859-8\u0026#34; iso-8859-8) (\u0026#34;iso-8859-8-i\u0026#34; iso-8859-8) ;; temporary solution (\u0026#34;iso-8859-9\u0026#34; iso-8859-9) (\u0026#34;iso-8859-15\u0026#34; iso-8859-15) (\u0026#34;iso-2022-cn\u0026#34; iso-2022-cn) (\u0026#34;iso-2022-cn-ext\u0026#34; iso-2022-cn-ext) (\u0026#34;gbk\u0026#34; gbk) (\u0026#34;gb2312\u0026#34; cn-gb-2312) ;; should be before cn-gb (\u0026#34;cn-gb\u0026#34; cn-gb-2312)   この設定では、 MIME の gb2312 を cn-gb-2312 に紐付けしているので、 gb2312 を gbk の紐付けに変更しているのが先ほどのコードとなります。  中国語圏とメールのやり取りしたときに何か問題がおこるかもですが、 自分にはそんな予定はないのでとりあえずこれで十分かな、と。 ","id":25,"section":"posts","summary":"Outlook は当初から評判が良くないため個人的には使用していません。 もうず〜〜〜〜〜と、 PC のメール環境は Mew を使用しています。 しかし、自分のメール送信・","tags":["mew","outlook"],"title":"Outlook で送信された日本語メールを Mew で受信すると文字化けする問題の対応","uri":"https://ifritjp.github.io/blog2/public/posts/2019/2019-02-07-outlook/","year":"2019"},{"content":" ここ十年ほどまともにゲームしてないけど、 ネット検索しているときになんとなく気になった記事を読んでみたら、 ものスゴく面白かった。  古めの記事だけど、載っけておく。    格ゲー“暗黒の10年”は、『鉄拳』を世界一売れる格闘ゲームへと鍛え上げた──世界市場に活路を拓いた戦略を訊く【バンダイナムコ原田勝弘インタビュー／西田宗千佳連載】    http://news.denfaminicogamer.jp/interview/180428    「久夛良木が面白かったからやってただけ」 プレイステーションの立役者に訊くその誕生秘話【丸山茂雄×川上量生】    http://news.denfaminicogamer.jp/interview/ps_history ","id":26,"section":"posts","summary":"ここ十年ほどまともにゲームしてないけど、 ネット検索しているときになんとなく気になった記事を読んでみたら、 ものスゴく面白かった。 古めの記事だけ","tags":["etc"],"title":"電ファミニコゲーマー","uri":"https://ifritjp.github.io/blog2/public/posts/2019/2019-02-03-interview/","year":"2019"},{"content":" 先日のデフォルト引数の指定し忘れ問題の対応を行なった。  詳しくは、次の記事を参照。  \u0026lt;https://qiita.com/dwarfJP/items/922c523d27a6d77fff6d\u0026gt; ","id":27,"section":"posts","summary":"先日のデフォルト引数の指定し忘れ問題の対応を行なった。 詳しくは、次の記事を参照。 \u0026lt;https://qiita.com/dwarfJP/items/922c523d27a6d77fff6d\u0026gt;","tags":["proglang"],"title":"デフォルト引数の問題の対応","uri":"https://ifritjp.github.io/blog2/public/posts/2019/2019-01-27-default-arg/","year":"2019"},{"content":" 関数をコールする際、引数を省略してコールできる機能をもつ言語が多く存在する。  ここでは、その機能を「デフォルト引数」と呼ぶ。  デフォルト引数の例として、Lua のサンプルを次に示す。 local function func( x, y ) print( x, y ) end func( \u0026#34;abc\u0026#34; ) // abc nil   Lua では関数コール時に省略された引数は、 nil として処理される。 上記の func( \u0026#34;abc\u0026#34; ) は、引数 x, y のうち y が省略され、 実行すると abc nil が表示される。  デフォルト引数は、引数が多い関数を呼び出す際に有効な機能である。 特に Lua は、引数の違いによって実行する関数を切り替える関数オーバーロードがないため、 デフォルト引数は良く使われる機能の一つである。  しかし、デフォルト引数は便利である一方、不具合を発生させるリスクにもなる。  そのリスクとは、意図してデフォルト引数を使用しているのか、 それとも、本来指定すべき引数を指定し忘れているのか、を判断出来ないということである。 タイプミス等で関数に渡す引数を間違えることが良くある。 それを判断できないというのはリスクが高い。  Lua の トランスコンパイラである LuneScript でも、同じ問題を抱えている。  次は LuneScript のデフォルト引数のサンプルである。 fn func( val: int! ): int { when! val { return val + 1; } return 0; } print( func( 1 ) ); // 2 print( func( nil ) ); // 0 print( func() ); // 0   このサンプルは、デフォルト引数を持つ func() の関数呼び出しを 3 パターン行なっている。    func( 1 )    func( nil )    func()    LuneScript は Lua と同じで、引数が省略されると nil が指定される。 よって、 func( nil ) と func() は同義である。 しかし、 func() が引数の指定忘れではないと、誰が保証できるだろうか？  また、 LuneScript では nilable は必ず省略可能なデフォルト引数になってしまう。  デフォルト引数をサポートする多くの言語では、 デフォルト引数はデフォルト値を定義する必要がある。 一方 LuneScript では、nilable は必ずデフォルト引数になってしまう。  「nil の時でも省略せずに明示すべき」としたくても、 現在の言語仕様ではそれが出来ない。  この辺りを解決する方法を検討している。  ただこれを解決するには、現状の言語仕様との互換を持たせるのは難しいかもしれない。 ","id":28,"section":"posts","summary":"関数をコールする際、引数を省略してコールできる機能をもつ言語が多く存在する。 ここでは、その機能を「デフォルト引数」と呼ぶ。 デフォルト引数の例","tags":["proglang"],"title":"デフォルト引数の問題","uri":"https://ifritjp.github.io/blog2/public/posts/2019/2019-01-23-default-arg/","year":"2019"},{"content":"  blog を始めるにあたって、 emacs から出来るだけ簡単に記事を更新できる環境にするために、 次の URL の情報をもとにいくつか調査。  \u0026lt;https://orgmode.org/worg/org-blog-wiki.html\u0026gt;  とりあえず org-mode + jekyll で構築してみた。  以下は、org-mode + jekyll で環境構築から記事を投稿するまでの流れをまとめたメモ。 使用するソフト     ruby    gem    bundler    jekyll    jekyll-org      jekyll は、 markdown でサイトを構築可能なツール。 markdown は書き慣れていないので org-mode で記事を書けるように jekyll plugin の jekyll-org を使用する。 setup     install ruby    install ruby-dev    install gem   $ gem install bundler jekyll jekyll-org  jekyll setup     blog のプロジェクトディレクトリ作成   $ cd blog_top $ jekyll new blog   初回は、ここで必要な gem がインストールされる。  blog ディレクトリが生成され、blog ディレクトリ以下に幾つかのファイルが作成される。    _config.yml を編集    次の項目を編集    title:    email:    description:    twitter_username:    github_username:      plugins の項目に jekyll-org を追加     plugins:- jekyll-org  jekyll-org の設定   \u0026lt;https://github.com/eggcaker/jekyll-org\u0026gt;  Gemfile に次を追加 # jekyll-org gem \u0026#39;jekyll-org\u0026#39;, \u0026#39;\u0026gt;= 1.0.2\u0026#39;   Gemfile 編集後、次のコマンドを実行 $ bundle install  github pages 用の設定   github pages の /blog に jekyll のディレクトリを作成した場合の設定    _config.yml を編集    次の項目を設定    baseurl: \u0026#34;/blog/site\u0026#34;    url: \u0026#34;https://XXXXXXXX.github.io\u0026#34;    destination: site        jekyll の変換後の html は _site 以下に出力されるが、 github pages は _site 以下にはアクセスできないようなので、 destination: site で出力先を site に変更する。 記事作成   _posts/ 以下に、次の名前のファイルを作成する YYYY-MM-DD-title.org   例えば 2019-01-01-hoge.org とする。  title は、記事のタイトルで無くてもよい。 title は、 記事の URL に使用される。  _posts/ の下にサブディレクトリを掘って、その中にファイルを作成しても良い。 記事のフォーマット   次のメタ情報を入れれば、後は普通の org-mode 通りに記載可能。 #+LAYOUT: post #+TITLE: org-mode で blog #+TAGS: org-mode jekyll   +TAGS はオプション。 ワンポイントネタ     URL を書くだけだとリンクにならない。    リンクにする場合は URL を \u0026lt;\u0026gt; で囲む。     変換   書いた記事は jekyll を使って html に変換する。 $ cd blog $ jekyll b  確認   jekyll は httpd サーバ機能を持つ。 $ cd blog $ jekyll s   この状態でブラウザで http://localhost:4000 にアクセスすれば、 変換後の内容を確認できる。  なお、記事を修正すれば動的に変換されるので、 記事を修正後にブラウザをリロードすれば、修正後の内容を確認できる。  httpd サーバを終了する場合は、 Ctrl-C。 ネットワークアクセス  $ jekyll s   このコマンドで起動した httpd サーバは、 localhost でしかアクセスできない。  つまり PC 外部からアクセス出来ない。  セキュリティという意味では安全であるが、不便だったりする。  PC 外部からアクセスしたい場合は、次のコマンドで httpd サーバを起動する。 $ jekyll s --host 0.0.0.0  ","id":29,"section":"posts","summary":"blog を始めるにあたって、 emacs から出来るだけ簡単に記事を更新できる環境にするために、 次の URL の情報をもとにいくつか調査。 \u0026lt;https://orgmode.org/worg/org-blog-wiki.html\u0026gt; とりあえず org-mode + jekyll で構築して","tags":["org-mode","jekyll"],"title":"org-mode で blog","uri":"https://ifritjp.github.io/blog2/public/posts/2019/2019-01-17-setup-jekyll/","year":"2019"}],"tags":[{"title":"emacs","uri":"https://ifritjp.github.io/blog2/public/tags/emacs/"},{"title":"etc","uri":"https://ifritjp.github.io/blog2/public/tags/etc/"},{"title":"jekyll","uri":"https://ifritjp.github.io/blog2/public/tags/jekyll/"},{"title":"lua","uri":"https://ifritjp.github.io/blog2/public/tags/lua/"},{"title":"mew","uri":"https://ifritjp.github.io/blog2/public/tags/mew/"},{"title":"org-mode","uri":"https://ifritjp.github.io/blog2/public/tags/org-mode/"},{"title":"outlook","uri":"https://ifritjp.github.io/blog2/public/tags/outlook/"},{"title":"proglang","uri":"https://ifritjp.github.io/blog2/public/tags/proglang/"}]}